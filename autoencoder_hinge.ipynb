{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":120,\n",
    "    \"dimRNA\":3633,\n",
    "    \"dimATAC\":4403,\n",
    "    \"n_hidden\":1024,\n",
    "    \"layer_sizes\":[1024, 1024, 1024, 256, 256],\n",
    "    \"nz\":128,\n",
    "    \"batchSize\":512,\n",
    "    \"lr\":1e-3,\n",
    "    \"lamb_kl\":1e-9,\n",
    "    \"lamb_anc\":1e-9,\n",
    "    \"clip_grad\":0.1,\n",
    "    \"weightDirName\": './checkpoint/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayueg/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes = load_raw.load_raw_cell_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [],
   "source": [
    "scicar_data, joint_index, keep_cells_idx = load_raw.merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scica|r_data)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tryout log cpm scicar_data\n",
    "nm.log_cpm(scicar_data)\n",
    "nm.log_cpm(scicar_data, obsm = \"mode2\", obs = \"mode2_obs\", var = \"mode2_var\")\n",
    "nm.hvg_by_sc(scicar_data, proportion = 0.06)\n",
    "nm.hvg_by_sc(scicar_data, obsm = \"mode2\", obs = \"mode2_obs\", \n",
    "             var = \"mode2_var\", proportion = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data.uns[\"mode2_obs\"] = np.array(scicar_data.uns[\"mode2_obs\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data.uns[\"mode2_var\"] = np.array(scicar_data.uns[\"mode2_var\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data.uns = {\"mode2_obs\": scicar_data.uns[\"mode2_obs\"], \"mode2_var\": scicar_data.uns[\"mode2_var\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_raw.train_test_split(scicar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1422x3633 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 68648 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3317x4403 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 15736 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.obsm[\"mode2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "    def __init__(self, adata):\n",
    "        self.rna_data, self.atac_data = self._load_merge_data(adata)\n",
    "\n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.atac_data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        rna_sample = self.rna_data.values[idx]\n",
    "        atac_sample = self.atac_data.values[idx]\n",
    "        #return a tensor that for a single observation\n",
    "        return torch.from_numpy(rna_sample).float(), torch.from_numpy(atac_sample).float()\n",
    "  \n",
    "    def _load_merge_data(self, adata):\n",
    "        rna_df = pd.DataFrame(data = adata.X.toarray(), index = np.array(adata.obs.index), columns = np.array(adata.var.index))\n",
    "        atac_df = pd.DataFrame(data = adata.obsm[\"mode2\"].toarray(), index = np.array(adata.uns[\"mode2_obs\"]), columns = np.array(adata.uns[\"mode2_var\"]))\n",
    "        return rna_df, atac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models(autoencoders) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input, n_latent, layer_sizes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_input] + layer_sizes + [n_latent]\n",
    "        self.encoder_layers = []\n",
    "        \n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx + 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.encoder_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx + 1])\n",
    "            self.endocer_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.encoder_layers.append(act1)\n",
    "            \n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_output, n_latent, layer_sizes, final_activation=None):\n",
    "        self.n_output = n_output\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_output] + layer_sizes + [n_latent]\n",
    "        self.decoder1_layers = []\n",
    "        for idx in range(len(self.layer_sizes) - 1, 1,  -1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx - 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.decoder1_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx - 1])\n",
    "            self.decoder1_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.decoder1_layers.append(act1)\n",
    "        self.decoder1 = nn.Sequential(self.decoder1_layers)\n",
    "        \n",
    "        self.n_inter = self.layer_sizes[len(self.layer_sizes)-2]\n",
    "        self.decoder21 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder21.weight)\n",
    "        self.decoder22 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder22.weight)\n",
    "        self.decoder23 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder23.weight)\n",
    "        \n",
    "        self.final_activations = nn.ModuleDict()\n",
    "        if final_activation is not None:\n",
    "            if isinstance(final_activation, list) or isinstance(final_activation, tuple):\n",
    "                for i, act in enumerate(final_activation):\n",
    "                    self.final_activations[f\"act{i+1}\"] = act\n",
    "            elif isinstance(final_activation, nn.Module):\n",
    "                self.final_activations[\"act1\"] = final_activation\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized type for final_activation: {type(final_activation)}\"\n",
    "                )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = self.decoder1(x)\n",
    "            \n",
    "            retval1 = self.decoder21(x)\n",
    "            if \"act1\" in self.final_activations.keys():\n",
    "                retval1 = self.final_activations[\"act1\"](retval1)\n",
    "            \n",
    "            retval2 = self.decoder22(x)\n",
    "            if \"act2\" in self.final_activations.keys():\n",
    "                retval2 = self.final_activations[\"act2\"](retval2)\n",
    "            \n",
    "            retval3 = self.decoder23(x)\n",
    "            if \"act3\" in self.final_activations.keys():\n",
    "                retval3 = self.final_activations[\"act3\"](retval3)\n",
    "                \n",
    "            return retval1, retval2, retval3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "46TuWH_Rgwnc"
   },
   "outputs": [],
   "source": [
    "class FC_VAE(nn.Module):\n",
    "    def __init__(self, n_input, nz, n_hidden=hyper[\"n_hidden\"], layer_sizes=hyper[\"layer_sizes\"]):\n",
    "        super(FC_VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.nz = nz\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        self.encoder_layers = []\n",
    "\n",
    "        self.encoder_layers.append(nn.Linear(n_input, self.layer_sizes[0]))\n",
    "        self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "        self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[0]))\n",
    "\n",
    "        for layer_idx in range(len(layer_sizes)-1):\n",
    "            if layer_idx == len(layer_sizes) - 2:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "            else:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *self.encoder_layers\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "        self.fc2 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "        self.decoder_layers.append(nn.Linear(nz, self.layer_sizes[-1]))\n",
    "        self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "        self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[-1]))\n",
    "\n",
    "        for layer_idx in range(len(self.layer_sizes)-1, 0, -1):\n",
    "            self.decoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx-1]))\n",
    "            self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "            self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx-1]))\n",
    "\n",
    "        self.decoder_layers.append(nn.Linear(self.layer_sizes[0], self.n_input))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            *self.decoder_layers\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc1(h), self.fc2(h)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #calculate std from log(var)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        res = self.decode(z)\n",
    "        return res, z, mu, logvar\n",
    "\n",
    "    def get_latent_var(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    n_svd = 100\n",
    "    proportion_neighbors = 0.1\n",
    "    \n",
    "    rna_inputs, atac_inputs = zip(*batch)\n",
    "    rna_inputs = torch.stack(rna_inputs)\n",
    "    atac_inputs = torch.stack(atac_inputs)\n",
    "    \n",
    "    n_svd = min([n_svd, min(rna_inputs.shape) - 1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors * rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "    )\n",
    "    \n",
    "    return rna_inputs, atac_inputs, torch.from_numpy(indices_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "def get_data_loaders(train_data, test_data):\n",
    "    train_set = Merge_Dataset(train_data)\n",
    "    test_set = Merge_Dataset(test_data)\n",
    "    #load data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_set, \n",
    "        batch_size=hyper[\"batchSize\"], \n",
    "        collate_fn=collate_fn, \n",
    "        drop_last=False, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, \n",
    "        batch_size=test_data.shape[0], \n",
    "        collate_fn=collate_fn, \n",
    "        drop_last=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = get_data_loaders(train_data=train_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qcs2gy4KwzCL"
   },
   "outputs": [],
   "source": [
    "#load basic models\n",
    "netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTphd8ob4oCQ",
    "outputId": "dfcd6557-66c5-48d9-b38b-5cdef05d1cc2"
   },
   "outputs": [],
   "source": [
    "#use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU\")\n",
    "    netRNA.cuda()\n",
    "    netATAC.cuda()\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "#setup optimizers for two nets\n",
    "opt_netRNA = optim.Adam(list(netRNA.parameters()), lr=hyper[\"lr\"])\n",
    "opt_netATAC = optim.Adam(list(netATAC.parameters()), lr=hyper[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureHingeLoss(nn.Module):\n",
    "    def __init__(self, margin, max_val, lamb_match, lamb_nn):\n",
    "        super(StructureHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.max_val = max_val\n",
    "        self.lamb_match = lamb_match\n",
    "        self.lamb_nn = lamb_nn\n",
    "        \n",
    "    def forward(self, rna_outputs, atac_outputs, nn_indices):\n",
    "        #rna_outputs: n_batch x n_latent\n",
    "        #atac_outputs: n_batch x n_latent\n",
    "        assert rna_outputs.shape[0] == atac_outputs.shape[0]\n",
    "        assert rna_outputs.shape[1] == atac_outputs.shape[1]\n",
    "        n_batch = rna_outputs.shape[0]\n",
    "        \n",
    "        #calculated pairwise L2 distance\n",
    "        #dist_rna_atac[i][j]: the L2 distance between RNA embedding i\n",
    "        #and ATAC embedding j (n_batch x n_batch)\n",
    "        #constraint for ensuring every rna embedding is close to matched atac embedding\n",
    "        dist_rna_atac = torch.cdist(rna_outputs, atac_outputs, p=2)\n",
    "        match_labels = torch.eye(n_batch)\n",
    "        match_mask = match_labels > 0\n",
    "        pos_match_dist = torch.masked_select(dist_rna_atac, match_mask).view(n_batch, 1)\n",
    "        neg_match_dist = torch.masked_select(dist_rna_atac, ~match_mask).view(n_batch, -1)\n",
    "        \n",
    "        loss_match_rna = torch.clamp(self.margin + pos_match_dist - neg_match_dist, 0, self.max_val)\n",
    "        loss_match_rna = loss_match_rna.mean()\n",
    "        #print(f\"loss_match_rna: {loss_match_rna}\")\n",
    "        \n",
    "        #constraint for ensuring every atac embedding is close to matched rna embedding\n",
    "        dist_atac_rna = dist_rna_atac.t()\n",
    "        pos_match_dist = torch.masked_select(dist_atac_rna, match_mask).view(n_batch, 1)\n",
    "        neg_match_dist = torch.masked_select(dist_atac_rna, ~match_mask).view(n_batch, -1)\n",
    "        \n",
    "        loss_match_atac = torch.clamp(self.margin + pos_match_dist - neg_match_dist, 0, self.max_val)\n",
    "        loss_match_atac = loss_match_rna.mean()\n",
    "        #print(f\"loss_match_atac: {loss_match_atac}\")\n",
    "        \n",
    "        #constraint for ensuring that every RNA embedding is close to \n",
    "        #the neighboring RNA embeddings.\n",
    "        nn_masked = torch.zeros(n_batch, n_batch)\n",
    "        nn_masked.scatter_(1, nn_indices, 1.)\n",
    "        nn_masked = nn_masked > 0\n",
    "        \n",
    "        dist_rna_rna = torch.cdist(rna_outputs, rna_outputs, p=2)\n",
    "        \n",
    "        #pos_rna_nn_dist: n_batch x n_neighbor\n",
    "        pos_rna_nn_dist = torch.masked_select(dist_rna_rna, nn_masked).view(n_batch, -1)\n",
    "        neg_rna_nn_dist = torch.masked_select(dist_rna_rna, ~nn_masked).view(n_batch, -1)\n",
    "        rna_nn_loss = torch.clamp(self.margin + pos_rna_nn_dist[...,None] - neg_rna_nn_dist[..., None, :], 0, self.max_val)\n",
    "        rna_nn_loss = rna_nn_loss.mean()\n",
    "        #print(f\"rna_nn_loss: {rna_nn_loss}\")\n",
    "        \n",
    "        #constraint for ensuring that every ATAC embedding is close to \n",
    "        #the neighboring ATAC embeddings.\n",
    "        dist_atac_atac = torch.cdist(atac_outputs, atac_outputs, p=2)\n",
    "        #pos_rna_nn_dist: n_batch x n_neighbor\n",
    "        pos_atac_nn_dist = torch.masked_select(dist_atac_atac, nn_masked).view(n_batch, -1)\n",
    "        neg_atac_nn_dist = torch.masked_select(dist_atac_atac, ~nn_masked).view(n_batch, -1)\n",
    "        atac_nn_loss = torch.clamp(self.margin + pos_atac_nn_dist[...,None] - neg_atac_nn_dist[..., None, :], 0, self.max_val)\n",
    "        atac_nn_loss = atac_nn_loss.mean()\n",
    "        #print(f\"atac_nn_loss: {atac_nn_loss}\")\n",
    "        \n",
    "        loss = (self.lamb_match * loss_match_rna \n",
    "                + self.lamb_match * loss_match_atac\n",
    "                + self.lamb_nn * rna_nn_loss \n",
    "                + self.lamb_nn * atac_nn_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LH_uoNVQ6v1O"
   },
   "outputs": [],
   "source": [
    "#set up loss function\n",
    "def basic_loss(recon_x, x, mu, logvar, lamb1):\n",
    "    MSE = nn.MSELoss()\n",
    "    lloss = MSE(recon_x, x)\n",
    "    #KL divergence\n",
    "    KL_loss = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    lloss = lloss + lamb1*KL_loss\n",
    "    return lloss\n",
    "\n",
    "#anchor loss for minimizing distance between paired observation\n",
    "def anchor_loss(embed_rna, embed_atac):\n",
    "    L1 = nn.L1Loss()\n",
    "    anc_loss = L1(embed_rna, embed_atac)\n",
    "    return anc_loss\n",
    "\n",
    "def hinge_loss(\n",
    "    margin, \n",
    "    max_val, \n",
    "    lamb_match,\n",
    "    lamb_nn, \n",
    "    embed_rna, \n",
    "    embed_atac, \n",
    "    nn_indices,\n",
    "):\n",
    "    Hinge_Loss = StructureHingeLoss(margin, max_val, lamb_match, lamb_nn)\n",
    "    loss = Hinge_Loss(embed_rna, embed_atac, nn_indices)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_criteria(rna_inputs, atac_inputs, rna_outputs, atac_outputs, proportion_neighbors=0.1, n_svd=100):\n",
    "    n_svd = min([n_svd, min(rna_inputs.shape)-1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors*rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "    )\n",
    "    _, indices_pred = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(rna_outputs).kneighbors(atac_outputs)\n",
    "    )\n",
    "    neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "    for i in range(rna_inputs.shape[0]):\n",
    "        _, pred_matches, true_matches = np.intersect1d(\n",
    "            indices_pred[i], indices_true[i], return_indices=True\n",
    "        )\n",
    "        neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "        neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "    neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * rna_inputs.shape[0])\n",
    "    area_under_curve = np.mean(neighbors_match_curve)\n",
    "    return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def train(epoch):\n",
    "    netRNA.train()\n",
    "    netATAC.train()\n",
    "    train_losses = []\n",
    "    for idx, samples in enumerate(train_loader):\n",
    "        rna_inputs, atac_inputs, nn_indices = Variable(samples[0]), Variable(samples[1]), samples[2]\n",
    "        if torch.cuda.is_available():\n",
    "            rna_inputs = rna_inputs.cuda()\n",
    "            atac_inputs = atac_inputs.cuda()\n",
    "            \n",
    "        opt_netATAC.zero_grad()\n",
    "        opt_netRNA.zero_grad()\n",
    "        recon_rna, z_rna, mu_rna, logvar_rna = netRNA(rna_inputs)\n",
    "        recon_atac, z_atac, mu_atac, logvar_atac = netATAC(atac_inputs)\n",
    "        rna_loss = basic_loss(recon_rna, rna_inputs, mu_rna, logvar_rna, lamb1=hyper[\"lamb_kl\"])\n",
    "        atac_loss = basic_loss(recon_atac, atac_inputs, mu_atac, logvar_atac, lamb1=hyper[\"lamb_kl\"])\n",
    "        \n",
    "        h_loss = hinge_loss(\n",
    "            margin=0.2, \n",
    "            max_val=1e6, \n",
    "            lamb_match=1,\n",
    "            lamb_nn=0.5,\n",
    "            embed_rna=z_rna, \n",
    "            embed_atac=z_atac, \n",
    "            nn_indices=nn_indices,\n",
    "        )\n",
    "        '''if epoch % 5 == 0:\n",
    "            print(f\"rna_loss: {rna_loss}\")\n",
    "            print(f\"atac_loss:{atac_loss}\")\n",
    "            print(f\"anc_loss: {anc_loss}\")\n",
    "            print(f\"hinge loss: {h_loss}\")'''\n",
    "        \n",
    "        #loss functions for each modalities\n",
    "        train_loss = rna_loss + atac_loss + h_loss\n",
    "        #train_loss = h_loss\n",
    "        #train_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss\n",
    "        #rain_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss + h_loss\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(netRNA.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "        nn.utils.clip_grad_norm_(netATAC.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "        opt_netRNA.step()\n",
    "        opt_netATAC.step()\n",
    "        train_losses.append(train_loss.item())\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(np.mean(train_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch):\n",
    "    #evaluating step\n",
    "    with torch.no_grad():\n",
    "        netRNA.eval()\n",
    "        netATAC.eval()\n",
    "        knn_acc = []\n",
    "        #mse_acc = []\n",
    "        for idx, samples in enumerate(test_loader):\n",
    "            rna_inputs = samples[0].float()\n",
    "            atac_inputs = samples[1].float()\n",
    "            rna_inputs = rna_inputs.to(device)\n",
    "            atac_inputs = atac_inputs.to(device)\n",
    "\n",
    "            _, output_rna, _, _ = netRNA(rna_inputs)\n",
    "            _, output_atac, _, _ = netATAC(atac_inputs)\n",
    "            knn_acc.append(knn_criteria(rna_inputs.cpu().detach(), atac_inputs.cpu().detach(), \n",
    "                                        output_rna.cpu().detach(), output_atac.cpu().detach()))\n",
    "        avg_knn_auc = np.mean(knn_acc)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", acc: \" + str(avg_knn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5ZvT1oFzCBBu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 2.5642493792942593\n",
      "Epoch: 0, acc: 0.08530599526387524\n",
      "Epoch: 5, train loss: 0.7254585112844195\n",
      "Epoch: 5, acc: 0.19060071915223933\n",
      "Epoch: 10, train loss: 0.5740096313612801\n",
      "Epoch: 10, acc: 0.18146362370125327\n",
      "Epoch: 15, train loss: 0.5465811320713588\n",
      "Epoch: 15, acc: 0.20300014506820613\n",
      "Epoch: 20, train loss: 0.5254029291016715\n",
      "Epoch: 20, acc: 0.2237362471747925\n",
      "Epoch: 25, train loss: 0.5062948976244245\n",
      "Epoch: 25, acc: 0.23392157118731247\n",
      "Epoch: 30, train loss: 0.493059550012861\n",
      "Epoch: 30, acc: 0.24246833663124398\n",
      "Epoch: 35, train loss: 0.482784628868103\n",
      "Epoch: 35, acc: 0.2479453977558715\n",
      "Epoch: 40, train loss: 0.4709799417427608\n",
      "Epoch: 40, acc: 0.2434553108098933\n",
      "Epoch: 45, train loss: 0.45106356910296846\n",
      "Epoch: 45, acc: 0.25968260116577185\n",
      "Epoch: 50, train loss: 0.4330393544265202\n",
      "Epoch: 50, acc: 0.2624669185834086\n",
      "Epoch: 55, train loss: 0.4121246039867401\n",
      "Epoch: 55, acc: 0.25749721364258243\n",
      "Epoch: 60, train loss: 0.3933856614998409\n",
      "Epoch: 60, acc: 0.26146278204972767\n",
      "Epoch: 65, train loss: 0.3709436740194048\n",
      "Epoch: 65, acc: 0.26688558783078736\n",
      "Epoch: 70, train loss: 0.3500657762799944\n",
      "Epoch: 70, acc: 0.26228532836864166\n",
      "Epoch: 75, train loss: 0.330108106136322\n",
      "Epoch: 75, acc: 0.26613383677165253\n",
      "Epoch: 80, train loss: 0.30966383644512724\n",
      "Epoch: 80, acc: 0.2669396544129971\n",
      "Epoch: 85, train loss: 0.2878227319036211\n",
      "Epoch: 85, acc: 0.2697918871997572\n",
      "Epoch: 90, train loss: 0.27195689933640615\n",
      "Epoch: 90, acc: 0.2649631130261142\n",
      "Epoch: 95, train loss: 0.2575788370200566\n",
      "Epoch: 95, acc: 0.2708098177427001\n",
      "Epoch: 100, train loss: 0.24415114521980286\n",
      "Epoch: 100, acc: 0.2694231467832171\n",
      "Epoch: 105, train loss: 0.23335170532975877\n",
      "Epoch: 105, acc: 0.26836323877715834\n",
      "Epoch: 110, train loss: 0.22068641654082707\n",
      "Epoch: 110, acc: 0.2682962132057325\n",
      "Epoch: 115, train loss: 0.21416605370385305\n",
      "Epoch: 115, acc: 0.26743107202268773\n"
     ]
    }
   ],
   "source": [
    "#train a toy model and see the scores\n",
    "max_iter = hyper[\"nEpochs\"]\n",
    "for epoch in range(max_iter):\n",
    "    train(epoch)\n",
    "    evaluate(epoch)\n",
    "  #set up log\n",
    "  #if epoch % 50 == 0:\n",
    "    #print(\"***saving checkpoints***\")\n",
    "    #path = \"{}Max_iter_{}lamb_anc_{}Epoch_{}params.pth\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]), str(epoch))\n",
    "    \n",
    "    #torch.save({\n",
    "    #    \"epoch\": epoch,\n",
    "    #    'netRNA_state_dict': netRNA.state_dict(),\n",
    "    #    'netATAC_state_dict': netATAC.state_dict(),\n",
    "    # }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_adata):\n",
    "    netRNA.eval()\n",
    "    netATAC.eval()\n",
    "    rna_inputs = Variable(torch.from_numpy(test_adata.X.toarray()).float())\n",
    "    atac_inputs = Variable(torch.from_numpy(test_adata.obsm[\"mode2\"].toarray()).float())\n",
    "    if torch.cuda.is_available():\n",
    "        rna_inputs = rna_inputs.cuda()\n",
    "        atac_inputs = atac_inputs.cuda()\n",
    "    _, z_rna, _, _ = netRNA(rna_inputs)\n",
    "    _, z_atac, _, _ = netATAC(atac_inputs)\n",
    "    test_adata.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "    test_adata.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "    knn_score, mse_score = metrics.knn_auc(test_adata), metrics.mse(test_adata)\n",
    "    return knn_score, mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2638007403655664\n",
      "0.9727991\n",
      "0.3449979906182975\n",
      "0.6296172\n"
     ]
    }
   ],
   "source": [
    "#test knn_auc plateau at around 0.09, seems that training starts to overfit\n",
    "test_knn_score, test_mse_score = model_eval(test_data)\n",
    "print(test_knn_score)\n",
    "print(test_mse_score)\n",
    "train_knn_score, train_mse_score = model_eval(train_data)\n",
    "print(train_knn_score)\n",
    "print(train_mse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log the metrics\n",
    "path = \"{}Max_iter_{}lamb_anc_{}metrics.txt\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]))\n",
    "'''torch.save({\n",
    "    \"num_iter\": hyper[\"nEpochs\"],\n",
    "    \"lamb_anc\": hyper[\"lamb_anc\"],\n",
    "    'knn_auc': knn_score,\n",
    "    'mse': mse_score,\n",
    "}, path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'a') as f:\n",
    "        print('nEpoch: ', hyper[\"nEpochs\"], 'lamb_anc:%.8f'%float(hyper[\"lamb_anc\"]) , ',knn_auc: %.8f' % float(knn_score), ', mse_score: %.8f' % float(mse_score), file=f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
