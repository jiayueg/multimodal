{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":400,\n",
    "    \"dimRNA\":60550,\n",
    "    \"dimATAC\":146713,\n",
    "    \"n_hidden\":1024,\n",
    "    \"nz\":128,\n",
    "    \"batchSize\":128,\n",
    "    \"lr\":1e-3,\n",
    "    \"lamb_kl\":0.0000001,\n",
    "    \"lamb_anc\":0.0001,\n",
    "    \"weightDirName\": './checkpoint/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qk7VlGW_FK6p"
   },
   "outputs": [],
   "source": [
    "rna_url = (\"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM3271040\"\n",
    "    \"&format=file&file=GSM3271040%5FRNA%5FsciCAR%5FA549%5Fgene%5Fcount.txt.gz\")\n",
    "rna_cells_url = (\n",
    "    \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM3271040\"\n",
    "    \"&format=file&file=GSM3271040%5FRNA%5FsciCAR%5FA549%5Fcell.txt.gz\"\n",
    ")\n",
    "rna_genes_url = (\n",
    "    \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM3271040\"\n",
    "    \"&format=file&file=GSM3271040%5FRNA%5FsciCAR%5FA549%5Fgene.txt.gz\"\n",
    ")\n",
    "atac_url = (\n",
    "    \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM3271041\"\n",
    "    \"&format=file&file=GSM3271041%5FATAC%5FsciCAR%5FA549%5Fpeak%5Fcount.txt.gz\"\n",
    ")\n",
    "atac_cells_url = (\n",
    "    \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM3271041\"\n",
    "    \"&format=file&file=GSM3271041%5FATAC%5FsciCAR%5FA549%5Fcell.txt.gz\"\n",
    ")\n",
    "atac_genes_url = (\n",
    "    \"https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSM3271041\"\n",
    "    \"&format=file&file=GSM3271041%5FATAC%5FsciCAR%5FA549%5Fpeak.txt.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "3eAoQ8zpG9-V",
    "outputId": "7e5c5eb5-b0d6-49b1-9cdd-d5432d5810dd"
   },
   "outputs": [],
   "source": [
    "rna_genes = pd.read_csv(rna_genes_url, low_memory=False, index_col=0)\n",
    "atac_genes =  pd.read_csv(atac_genes_url, low_memory=False, index_col=1)\n",
    "rna_cells = pd.read_csv(rna_cells_url, low_memory=False, index_col=0)\n",
    "atac_cells = pd.read_csv(atac_cells_url, low_memory=False, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-0njer-qFpMl"
   },
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "  rna_file = os.path.join(tempdir, \"rna.mtx.gz\")\n",
    "  scprep.io.download.download_url(rna_url, rna_file)\n",
    "  rna_data = scprep.io.load_mtx(rna_file, cell_axis=\"col\").tocsr()\n",
    "  atac_file = os.path.join(tempdir, \"atac.mtx.gz\")\n",
    "  scprep.io.download.download_url(atac_url, atac_file)\n",
    "  atac_data = scprep.io.load_mtx(atac_file, cell_axis=\"col\").tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcF3DwMRMGPl"
   },
   "source": [
    " **2. select the joint sub-datasets and store them into csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HLB4X7nxMFiv"
   },
   "outputs": [],
   "source": [
    "def create_joint_dataset(X, Y, X_index=None, X_columns=None, Y_index=None, Y_columns=None):\n",
    "    if X_index is None:\n",
    "        X_index = X.index\n",
    "    if X_columns is None:\n",
    "        X_columns = X.columns\n",
    "    if Y_index is None:\n",
    "        Y_index = Y.index\n",
    "    if Y_columns is None:\n",
    "        Y_columns = Y.columns\n",
    "    joint_index = np.sort(np.intersect1d(X_index, Y_index))\n",
    "    try:\n",
    "        X = X.loc[joint_index]\n",
    "        Y = Y.loc[joint_index]\n",
    "    except AttributeError:\n",
    "        x_keep_idx = np.isin(X_index, joint_index)\n",
    "        y_keep_idx = np.isin(Y_index, joint_index)\n",
    "        X = X[x_keep_idx]\n",
    "        Y = Y[y_keep_idx]\n",
    "        X_index_sub = scprep.utils.toarray(X_index[x_keep_idx])\n",
    "        Y_index_sub = scprep.utils.toarray(Y_index[y_keep_idx])\n",
    "        X = X[np.argsort(X_index_sub)]\n",
    "        Y = Y[np.argsort(Y_index_sub)]\n",
    "        # check order is correct\n",
    "        assert (X_index_sub[np.argsort(X_index_sub)] == joint_index).all()\n",
    "        assert (Y_index_sub[np.argsort(Y_index_sub)] == joint_index).all()\n",
    "    adata = anndata.AnnData(\n",
    "        scprep.utils.to_array_or_spmatrix(X).tocsr(),\n",
    "        obs = pd.DataFrame(index = joint_index),\n",
    "        var = pd.DataFrame(index = X_columns),\n",
    "    )\n",
    "    adata.obsm[\"mode2\"] = scprep.utils.to_array_or_spmatrix(Y).tocsr()\n",
    "    adata.uns[\"mode2_obs\"] = joint_index\n",
    "    adata.uns[\"mode2_var\"] = scprep.utils.toarray(Y_columns)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ms5wS5iNUob8"
   },
   "outputs": [],
   "source": [
    "def subset_mode2_genes(adata, keep_genes):\n",
    "  adata.obsm[\"mode2\"] = adata.obsm[\"mode2\"][:, keep_genes]\n",
    "  adata.uns[\"mode2_var\"] = adata.uns[\"mode2_var\"][keep_genes]\n",
    "  if \"mode2_varnames\" in adata.uns:\n",
    "    for varname in adata.uns[\"mode2_varnames\"]:\n",
    "      adata.uns[varname] = adata.uns[varname][keep_genes]\n",
    "  return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2caFwIv5P5py"
   },
   "outputs": [],
   "source": [
    "def filter_joint_data_empty_cells(adata):\n",
    "    assert np.all(adata.uns[\"mode2_obs\"] == adata.obs.index)\n",
    "    #filter out cells\n",
    "    n_cells_mode1 = scprep.utils.toarray(adata.X.sum(axis = 1)).flatten()\n",
    "    n_cells_mode2 = scprep.utils.toarray(adata.obsm[\"mode2\"].sum(axis = 1)).flatten()\n",
    "    keep_cells = np.minimum(n_cells_mode1, n_cells_mode2) > 1\n",
    "    adata.uns[\"mode2_obs\"] = adata.uns[\"mode2_obs\"][keep_cells]\n",
    "    adata = adata[keep_cells, :].copy()\n",
    "    #filter out genes\n",
    "    sc.pp.filter_genes(adata, min_counts=1)\n",
    "    n_genes_mode2 = scprep.utils.toarray(adata.obsm[\"mode2\"].sum(axis=0)).flatten()\n",
    "    keep_genes_mode2 = n_genes_mode2 > 0\n",
    "    adata = subset_mode2_genes(adata, keep_genes_mode2)\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5Jg8s2v1T6Z6"
   },
   "outputs": [],
   "source": [
    "def merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes):\n",
    "  scicar_data = create_joint_dataset(\n",
    "      rna_data, atac_data, \n",
    "      X_index=rna_cells.index, \n",
    "      X_columns=rna_genes.index, \n",
    "      Y_index=atac_cells.index,\n",
    "      Y_columns=atac_genes.index)\n",
    "\n",
    "  scicar_data.obs = rna_cells.loc[scicar_data.obs.index]\n",
    "  scicar_data.var = rna_genes\n",
    "  for key in atac_cells.columns:\n",
    "      scicar_data.obs[key] = atac_cells[key]\n",
    "  scicar_data.uns[\"mode2_varnames\"] = []\n",
    "  for key in atac_genes.columns:\n",
    "      varname = \"mode2_var_{}\".format(key)\n",
    "      scicar_data.uns[varname] = atac_genes[key].values\n",
    "      scicar_data.uns[\"mode2_varnames\"].append(varname)\n",
    "  scicar_data = filter_joint_data_empty_cells(scicar_data)\n",
    "  return scicar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [],
   "source": [
    "scicar_data = merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scicar_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OPYRsfpEiH8A"
   },
   "outputs": [],
   "source": [
    "class RNA_Dataset(Dataset):\n",
    "  def __init__(self, adata):\n",
    "    self.rna_data = self._load_rna_data(adata)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.rna_data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    rna_sample = self.rna_data.values[idx]\n",
    "    #return a tensor that for a single observation\n",
    "    return torch.from_numpy(rna_sample).float()\n",
    "  \n",
    "  def _load_rna_data(self, adata):\n",
    "     rna_df = pd.DataFrame(data = adata.X.toarray(), index = np.array(adata.obs.index), columns = np.array(adata.var.index))\n",
    "     return rna_df\n",
    "\n",
    "class ATAC_Dataset(Dataset):\n",
    "  def __init__(self, adata):\n",
    "    self.atac_data = self._load_atac_data(adata)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.atac_data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    atac_sample = self.atac_data.values[idx]\n",
    "    #return a tensor that for a single observation\n",
    "    return torch.from_numpy(atac_sample).float()\n",
    "  \n",
    "  def _load_atac_data(self, adata):\n",
    "    atac_df = pd.DataFrame(data = adata.obsm[\"mode2\"].toarray(), index = np.array(adata.uns[\"mode2_obs\"]), columns = np.array(adata.uns[\"mode2_var\"]))\n",
    "    return atac_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "  def __init__(self, adata):\n",
    "    self.rna_data, self.atac_data = self._load_merge_data(adata)\n",
    "\n",
    "  def __len__(self):\n",
    "    #assert(len(self.rna_data) == len(self.atac_data))\n",
    "    return len(self.atac_data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    rna_sample = self.rna_data.values[idx]\n",
    "    atac_sample = self.atac_data.values[idx]\n",
    "    #return a tensor that for a single observation\n",
    "    return {\"rna_tensor\": torch.from_numpy(rna_sample).float(), \"atac_tensor\": torch.from_numpy(atac_sample).float()}\n",
    "  \n",
    "  def _load_merge_data(self, adata):\n",
    "    rna_df = pd.DataFrame(data = adata.X.toarray(), index = np.array(adata.obs.index), columns = np.array(adata.var.index))\n",
    "    atac_df = pd.DataFrame(data = adata.obsm[\"mode2\"].toarray(), index = np.array(adata.uns[\"mode2_obs\"]), columns = np.array(adata.uns[\"mode2_var\"]))\n",
    "    return rna_df, atac_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XSvuozlHixbS"
   },
   "outputs": [],
   "source": [
    "#test datasets\n",
    "def test_loader(scicar_data):\n",
    "  merge_dataset = Merge_Dataset(scicar_data)\n",
    "  \n",
    "  print(merge_dataset[0])\n",
    "  print(len(merge_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models(autoencoders) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "46TuWH_Rgwnc"
   },
   "outputs": [],
   "source": [
    "class FC_VAE(nn.Module):\n",
    "  def __init__(self, n_input, nz, n_hidden = hyper[\"n_hidden\"]):\n",
    "    super(FC_VAE, self).__init__()\n",
    "    self.n_input = n_input\n",
    "    self.nz = nz\n",
    "    self.n_hidden = n_hidden\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(n_input, n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "    )\n",
    "    self.fc1 = nn.Linear(n_hidden, nz)\n",
    "    self.fc2 = nn.Linear(n_hidden, nz)\n",
    "\n",
    "    self.decoder = nn.Sequential(\n",
    "        nn.Linear(nz, n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(n_hidden, n_hidden),\n",
    "        nn.BatchNorm1d(n_hidden),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Linear(n_hidden, n_input),\n",
    "    )\n",
    "  def encode(self, x):\n",
    "    h = self.encoder(x)\n",
    "    return self.fc1(h), self.fc2(h)\n",
    "\n",
    "  def reparametrize(self, mu, logvar):\n",
    "    #calculate std from log(var)\n",
    "    std = logvar.mul(0.5).exp_()\n",
    "    if torch.cuda.is_available():\n",
    "      eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "    else:\n",
    "      eps = torch.FloatTensor(std.size()).normal_()\n",
    "    eps = Variable(eps)\n",
    "    return eps.mul(std).add_(mu)\n",
    "\n",
    "  def decode(self, z):\n",
    "    return self.decoder(z)\n",
    "\n",
    "  def forward(self, x):\n",
    "    mu, logvar = self.encode(x)\n",
    "    z = self.reparametrize(mu, logvar)\n",
    "    res = self.decode(z)\n",
    "    return res, z, mu, logvar\n",
    "\n",
    "  def get_latent_var(self, x):\n",
    "    mu, logvar = self.encode(x)\n",
    "    z = self.reparametrize(mu, logvar)\n",
    "    return z\n",
    "    \n",
    "  def generate(self, z):\n",
    "    return self.decode(z)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "merge_dataset = Merge_Dataset(scicar_data)\n",
    "train_len = int(len(merge_dataset)*0.8)\n",
    "lengths = [train_len, len(merge_dataset)-train_len]\n",
    "trainset, testset = random_split(merge_dataset, lengths)\n",
    "#netRNA = FC_VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qcs2gy4KwzCL"
   },
   "outputs": [],
   "source": [
    "#load data loader\n",
    "train_loader = DataLoader(trainset, batch_size=hyper[\"batchSize\"], drop_last=False, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=hyper[\"batchSize\"], drop_last=False, shuffle=False)\n",
    "\n",
    "#load basic models\n",
    "netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"])\n",
    "netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTphd8ob4oCQ",
    "outputId": "dfcd6557-66c5-48d9-b38b-5cdef05d1cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "#use GPU\n",
    "if torch.cuda.is_available():\n",
    "  print(\"using GPU\")\n",
    "  netRNA.cuda()\n",
    "  netATAC.cuda()\n",
    "\n",
    "#setup optimizers for two nets\n",
    "opt_netRNA = optim.Adam(list(netRNA.parameters()), lr=hyper[\"lr\"])\n",
    "opt_netATAC = optim.Adam(list(netATAC.parameters()), lr=hyper[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LH_uoNVQ6v1O"
   },
   "outputs": [],
   "source": [
    "#set up loss function\n",
    "def basic_loss(recon_x, x, mu, logvar, lamb1):\n",
    "  MSE = nn.MSELoss()\n",
    "  lloss = MSE(recon_x, x)\n",
    "  #KL divergence\n",
    "  KL_loss = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  lloss = lloss + lamb1*KL_loss\n",
    "  return lloss\n",
    "\n",
    "#anchor loss for minimizing distance between paired observation\n",
    "def anchor_loss(embed_rna, embed_atac):\n",
    "  L1 = nn.L1Loss(reduction = 'sum')\n",
    "  anc_loss = L1(embed_rna, embed_atac)\n",
    "  return anc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def train(epoch):\n",
    "  netRNA.train()\n",
    "  netATAC.train()\n",
    "  train_loss = 0\n",
    "  for idx, samples in enumerate(train_loader):\n",
    "    rna_inputs, atac_inputs = Variable(samples[\"rna_tensor\"]), Variable(samples[\"atac_tensor\"])\n",
    "    if torch.cuda.is_available():\n",
    "      rna_inputs = rna_inputs.cuda()\n",
    "      atac_inputs = atac_inputs.cuda()\n",
    "      opt_netATAC.zero_grad()\n",
    "      opt_netRNA.zero_grad()\n",
    "\n",
    "      recon_rna, z_rna, mu_rna, logvar_rna = netRNA(rna_inputs)\n",
    "      recon_atac, z_atac, mu_atac, logvar_atac = netATAC(atac_inputs)\n",
    "      rna_loss = basic_loss(recon_rna, rna_inputs, mu_rna, logvar_rna, lamb1=hyper[\"lamb_kl\"])\n",
    "      atac_loss = basic_loss(recon_atac, atac_inputs, mu_atac, logvar_atac, lamb1=hyper[\"lamb_kl\"])\n",
    "      anc_loss = anchor_loss(z_rna, z_atac)\n",
    "      \n",
    "      #loss functions for each modalities\n",
    "      train_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"]*anc_loss\n",
    "      train_loss.backward()\n",
    "      opt_netRNA.step()\n",
    "      opt_netATAC.step()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5ZvT1oFzCBBu"
   },
   "outputs": [],
   "source": [
    "#train a toy model and see the scores\n",
    "max_iter = hyper[\"nEpochs\"]\n",
    "for epoch in range(max_iter):\n",
    "  train(epoch)\n",
    "  #set up log\n",
    "  #if epoch % 50 == 0:\n",
    "    #print(\"***saving checkpoints***\")\n",
    "    #path = \"{}Max_iter_{}lamb_anc_{}Epoch_{}params.pth\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]), str(epoch))\n",
    "    \n",
    "    #torch.save({\n",
    "    #    \"epoch\": epoch,\n",
    "    #    'netRNA_state_dict': netRNA.state_dict(),\n",
    "    #    'netATAC_state_dict': netATAC.state_dict(),\n",
    "    # }, path)\n",
    "\n",
    "del trainset\n",
    "del testset\n",
    "#start to evaluate the data\n",
    "netRNA.eval()\n",
    "rna_inputs = Variable(torch.from_numpy(merge_dataset.rna_data.values).float())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  rna_inputs = rna_inputs.cuda()\n",
    "\n",
    "_, z_rna, _, _ = netRNA(rna_inputs)\n",
    "scicar_data.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "del z_rna\n",
    "del netRNA\n",
    "del rna_inputs\n",
    "del train_loader\n",
    "del test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nzgT6p2p4bTv"
   },
   "outputs": [],
   "source": [
    "netATAC.eval()\n",
    "atac_inputs = Variable(torch.from_numpy(merge_dataset.atac_data.values).float())\n",
    "\n",
    "del merge_dataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  atac_inputs = atac_inputs.cuda()\n",
    "\n",
    "_, z_atac, _, _ = netATAC(atac_inputs)\n",
    "scicar_data.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "del atac_inputs\n",
    "del netATAC\n",
    "del z_atac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Il_OR3M4xNgE"
   },
   "outputs": [],
   "source": [
    "#KNN-AUC\n",
    "def knn_auc(adata, proportion_neighbors=0.1, n_svd=100):\n",
    "  n_svd = min([n_svd, min(adata.X.shape)-1])\n",
    "  n_neighbors = int(np.ceil(proportion_neighbors*adata.X.shape[0]))\n",
    "  X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(adata.X)\n",
    "  _, indices_true = (\n",
    "      sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "  )\n",
    "  _, indices_pred = (\n",
    "      sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(adata.obsm[\"aligned\"]).kneighbors(adata.obsm[\"mode2_aligned\"])\n",
    "  )\n",
    "  neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "  for i in range(adata.shape[0]):\n",
    "    _, pred_matches, true_matches = np.intersect1d(\n",
    "        indices_pred[i], indices_true[i], return_indices=True\n",
    "    )\n",
    "    neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "    neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "    neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * adata.shape[0])\n",
    "    area_under_curve = np.mean(neighbors_match_curve)\n",
    "    return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "d-imsgQExRa0"
   },
   "outputs": [],
   "source": [
    "#MSE\n",
    "def _square(X):\n",
    "  if sparse.issparse(X):\n",
    "        X.data = X.data ** 2\n",
    "        return X\n",
    "  else:\n",
    "        return scprep.utils.toarray(X) ** 2\n",
    "\n",
    "def mse(adata):\n",
    "  X=scprep.utils.toarray(adata.obsm[\"aligned\"])\n",
    "  Y=scprep.utils.toarray(adata.obsm[\"mode2_aligned\"])\n",
    "\n",
    "  X_shuffled = X[np.random.permutation(np.arange(X.shape[0])), :]\n",
    "  error_random = np.mean(np.sum(_square(X_shuffled - Y)))\n",
    "  error_abs = np.mean(np.sum(_square(X - Y)))\n",
    "  return error_abs/error_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.121189098180982e-06\n",
      "0.97133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'torch.save({\\n    \"num_iter\": hyper[\"nEpochs\"],\\n    \"lamb_anc\": hyper[\"lamb_anc\"],\\n    \\'knn_auc\\': knn_score,\\n    \\'mse\\': mse_score,\\n}, path)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_score = knn_auc(scicar_data)\n",
    "mse_score = mse(scicar_data)\n",
    "print(knn_score)\n",
    "print(mse_score)\n",
    "\n",
    "#log the metrics\n",
    "path = \"{}Max_iter_{}lamb_anc_{}metrics.txt\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]))\n",
    "'''torch.save({\n",
    "    \"num_iter\": hyper[\"nEpochs\"],\n",
    "    \"lamb_anc\": hyper[\"lamb_anc\"],\n",
    "    'knn_auc': knn_score,\n",
    "    'mse': mse_score,\n",
    "}, path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'a') as f:\n",
    "        print('nEpoch: ', hyper[\"nEpochs\"], 'lamb_anc:%.8f'%float(hyper[\"lamb_anc\"]) , ',knn_auc: %.8f' % float(knn_score), ', mse_score: %.8f' % float(mse_score), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
