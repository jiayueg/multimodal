{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":60,\n",
    "    \"dimRNA\":3633,\n",
    "    \"dimATAC\":4403,\n",
    "    \"n_hidden\":1024,\n",
    "    \"layer_sizes\":[1024, 1024, 1024, 256, 256],\n",
    "    \"nz\":128,\n",
    "    \"batchSize\":128,\n",
    "    \"lr\":1e-3,\n",
    "    \"lamb_kl\":1e-9,\n",
    "    \"lamb_anc\":1e-9,\n",
    "    \"clip_grad\":0.1,\n",
    "    \"weightDirName\": './checkpoint/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes = load_raw.load_raw_cell_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [],
   "source": [
    "scicar_data, joint_index, keep_cells_idx = load_raw.merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scica|r_data)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tryout log cpm scicar_data\n",
    "nm.log_cpm(scicar_data)\n",
    "nm.log_cpm(scicar_data, obsm = \"mode2\", obs = \"mode2_obs\", var = \"mode2_var\")\n",
    "nm.hvg_by_sc(scicar_data, proportion = 0.06)\n",
    "nm.hvg_by_sc(scicar_data, obsm = \"mode2\", obs = \"mode2_obs\", \n",
    "             var = \"mode2_var\", proportion = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data.uns[\"mode2_obs\"] = np.array(scicar_data.uns[\"mode2_obs\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data.uns[\"mode2_var\"] = np.array(scicar_data.uns[\"mode2_var\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data.uns = {\"mode2_obs\": scicar_data.uns[\"mode2_obs\"], \"mode2_var\": scicar_data.uns[\"mode2_var\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_raw.train_test_split(scicar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1422x3633 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 68399 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3317x4403 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 15934 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.obsm[\"mode2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "    def __init__(self, adata):\n",
    "        self.rna_data, self.atac_data = self._load_merge_data(adata)\n",
    "\n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.atac_data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        rna_sample = self.rna_data.values[idx]\n",
    "        atac_sample = self.atac_data.values[idx]\n",
    "        #return a tensor that for a single observation\n",
    "        return {\"rna_tensor\": torch.from_numpy(rna_sample).float(), \"atac_tensor\": torch.from_numpy(atac_sample).float()}\n",
    "  \n",
    "    def _load_merge_data(self, adata):\n",
    "        rna_df = pd.DataFrame(data = adata.X.toarray(), index = np.array(adata.obs.index), columns = np.array(adata.var.index))\n",
    "        atac_df = pd.DataFrame(data = adata.obsm[\"mode2\"].toarray(), index = np.array(adata.uns[\"mode2_obs\"]), columns = np.array(adata.uns[\"mode2_var\"]))\n",
    "        return rna_df, atac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models(autoencoders) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "46TuWH_Rgwnc"
   },
   "outputs": [],
   "source": [
    "class FC_VAE(nn.Module):\n",
    "    def __init__(self, n_input, nz, n_hidden=hyper[\"n_hidden\"], layer_sizes=hyper[\"layer_sizes\"]):\n",
    "        super(FC_VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.nz = nz\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        self.encoder_layers = []\n",
    "\n",
    "        self.encoder_layers.append(nn.Linear(n_input, self.layer_sizes[0]))\n",
    "        self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "        self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[0]))\n",
    "\n",
    "        for layer_idx in range(len(layer_sizes)-1):\n",
    "            if layer_idx == len(layer_sizes) - 2:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "            else:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *self.encoder_layers\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "        self.fc2 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "        self.decoder_layers.append(nn.Linear(nz, self.layer_sizes[-1]))\n",
    "        self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "        self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[-1]))\n",
    "\n",
    "        for layer_idx in range(len(self.layer_sizes)-1, 0, -1):\n",
    "            self.decoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx-1]))\n",
    "            self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "            self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx-1]))\n",
    "\n",
    "        self.decoder_layers.append(nn.Linear(self.layer_sizes[0], self.n_input))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            *self.decoder_layers\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc1(h), self.fc2(h)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #calculate std from log(var)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        res = self.decode(z)\n",
    "        return res, z, mu, logvar\n",
    "\n",
    "    def get_latent_var(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "def get_data_loaders(train_data, test_data):\n",
    "    train_set = Merge_Dataset(train_data)\n",
    "    test_set = Merge_Dataset(test_data)\n",
    "    #load data loader\n",
    "    train_loader = DataLoader(train_set, batch_size=hyper[\"batchSize\"], drop_last=False, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=test_data.shape[0], drop_last=False, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = get_data_loaders(train_data=train_data, test_data=sub_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "qcs2gy4KwzCL"
   },
   "outputs": [],
   "source": [
    "#load basic models\n",
    "netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTphd8ob4oCQ",
    "outputId": "dfcd6557-66c5-48d9-b38b-5cdef05d1cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "#use GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU\")\n",
    "    netRNA.cuda()\n",
    "    netATAC.cuda()\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "#setup optimizers for two nets\n",
    "opt_netRNA = optim.Adam(list(netRNA.parameters()), lr=hyper[\"lr\"])\n",
    "opt_netATAC = optim.Adam(list(netATAC.parameters()), lr=hyper[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "LH_uoNVQ6v1O"
   },
   "outputs": [],
   "source": [
    "#set up loss function\n",
    "def basic_loss(recon_x, x, mu, logvar, lamb1):\n",
    "    MSE = nn.MSELoss()\n",
    "    lloss = MSE(recon_x, x)\n",
    "    #KL divergence\n",
    "    KL_loss = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    lloss = lloss + lamb1*KL_loss\n",
    "    return lloss\n",
    "\n",
    "#anchor loss for minimizing distance between paired observation\n",
    "def anchor_loss(embed_rna, embed_atac):\n",
    "    L1 = nn.L1Loss()\n",
    "    anc_loss = L1(embed_rna, embed_atac)\n",
    "    return anc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_criteria(rna_inputs, atac_inputs, rna_outputs, atac_outputs, proportion_neighbors=0.1, n_svd=100):\n",
    "    n_svd = min([n_svd, min(rna_inputs.shape)-1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors*rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(rna_inputs).kneighbors(rna_inputs)\n",
    "    )\n",
    "    _, indices_pred = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(rna_outputs).kneighbors(atac_outputs)\n",
    "    )\n",
    "    neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "    for i in range(rna_inputs.shape[0]):\n",
    "        _, pred_matches, true_matches = np.intersect1d(\n",
    "            indices_pred[i], indices_true[i], return_indices=True\n",
    "        )\n",
    "        neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "        neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "    neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * rna_inputs.shape[0])\n",
    "    area_under_curve = np.mean(neighbors_match_curve)\n",
    "    return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def train(epoch):\n",
    "    netRNA.train()\n",
    "    netATAC.train()\n",
    "    train_losses = []\n",
    "    for idx, samples in enumerate(train_loader):\n",
    "        rna_inputs, atac_inputs = Variable(samples[\"rna_tensor\"]), Variable(samples[\"atac_tensor\"])\n",
    "        if torch.cuda.is_available():\n",
    "            rna_inputs = rna_inputs.cuda()\n",
    "            atac_inputs = atac_inputs.cuda()\n",
    "            \n",
    "        opt_netATAC.zero_grad()\n",
    "        opt_netRNA.zero_grad()\n",
    "        recon_rna, z_rna, mu_rna, logvar_rna = netRNA(rna_inputs)\n",
    "        recon_atac, z_atac, mu_atac, logvar_atac = netATAC(atac_inputs)\n",
    "        rna_loss = basic_loss(recon_rna, rna_inputs, mu_rna, logvar_rna, lamb1=hyper[\"lamb_kl\"])\n",
    "        atac_loss = basic_loss(recon_atac, atac_inputs, mu_atac, logvar_atac, lamb1=hyper[\"lamb_kl\"])\n",
    "        anc_loss = anchor_loss(z_rna, z_atac)\n",
    "        \n",
    "        #loss functions for each modalities\n",
    "        train_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(netRNA.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "        nn.utils.clip_grad_norm_(netATAC.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "        opt_netRNA.step()\n",
    "        opt_netATAC.step()\n",
    "        train_losses.append(train_loss.item())\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(np.mean(train_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(epoch):\n",
    "    #evaluating step\n",
    "    with torch.no_grad():\n",
    "        netRNA.eval()\n",
    "        netATAC.eval()\n",
    "        knn_acc = []\n",
    "        #mse_acc = []\n",
    "        for idx, samples in enumerate(test_loader):\n",
    "            rna_inputs = samples[\"rna_tensor\"].float()\n",
    "            atac_inputs = samples[\"atac_tensor\"].float()\n",
    "            rna_inputs = rna_inputs.to(device)\n",
    "            atac_inputs = atac_inputs.to(device)\n",
    "\n",
    "            _, output_rna, _, _ = netRNA(rna_inputs)\n",
    "            _, output_atac, _, _ = netATAC(atac_inputs)\n",
    "            knn_acc.append(knn_criteria(rna_inputs.cpu().detach(), atac_inputs.cpu().detach(), \n",
    "                                        output_rna.cpu().detach(), output_atac.cpu().detach()))\n",
    "        avg_knn_auc = np.mean(knn_acc)\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", acc: \" + str(avg_knn_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "id": "5ZvT1oFzCBBu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 0.7157778785778925\n",
      "Epoch: 0, acc: 0.1630443084895937\n",
      "Epoch: 20, train loss: 0.47401968103188735\n",
      "Epoch: 20, acc: 0.12240874636229951\n",
      "Epoch: 40, train loss: 0.46023509479486024\n",
      "Epoch: 40, acc: 0.16551815352795224\n"
     ]
    }
   ],
   "source": [
    "#train a toy model and see the scores\n",
    "max_iter = hyper[\"nEpochs\"]\n",
    "for epoch in range(max_iter):\n",
    "    train(epoch)\n",
    "    evaluate(epoch)\n",
    "  #set up log\n",
    "  #if epoch % 50 == 0:\n",
    "    #print(\"***saving checkpoints***\")\n",
    "    #path = \"{}Max_iter_{}lamb_anc_{}Epoch_{}params.pth\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]), str(epoch))\n",
    "    \n",
    "    #torch.save({\n",
    "    #    \"epoch\": epoch,\n",
    "    #    'netRNA_state_dict': netRNA.state_dict(),\n",
    "    #    'netATAC_state_dict': netATAC.state_dict(),\n",
    "    # }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_adata):\n",
    "    netRNA.eval()\n",
    "    netATAC.eval()\n",
    "    rna_inputs = Variable(torch.from_numpy(test_adata.X.toarray()).float())\n",
    "    atac_inputs = Variable(torch.from_numpy(test_adata.obsm[\"mode2\"].toarray()).float())\n",
    "    if torch.cuda.is_available():\n",
    "        rna_inputs = rna_inputs.cuda()\n",
    "        atac_inputs = atac_inputs.cuda()\n",
    "    _, z_rna, _, _ = netRNA(rna_inputs)\n",
    "    _, z_atac, _, _ = netATAC(atac_inputs)\n",
    "    test_adata.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "    test_adata.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "    knn_score, mse_score = metrics.knn_auc(test_adata), metrics.mse(test_adata)\n",
    "    return knn_score, mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0958825348422088\n",
      "0.9997422\n",
      "0.09558056510033025\n",
      "1.0048813\n"
     ]
    }
   ],
   "source": [
    "#test knn_auc plateau at around 0.09, seems that training starts to overfit\n",
    "test_knn_score, test_mse_score = model_eval(test_data)\n",
    "print(test_knn_score)\n",
    "print(test_mse_score)\n",
    "train_knn_score, train_mse_score = model_eval(train_data)\n",
    "print(train_knn_score)\n",
    "print(train_mse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log the metrics\n",
    "path = \"{}Max_iter_{}lamb_anc_{}metrics.txt\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]))\n",
    "'''torch.save({\n",
    "    \"num_iter\": hyper[\"nEpochs\"],\n",
    "    \"lamb_anc\": hyper[\"lamb_anc\"],\n",
    "    'knn_auc': knn_score,\n",
    "    'mse': mse_score,\n",
    "}, path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'a') as f:\n",
    "        print('nEpoch: ', hyper[\"nEpochs\"], 'lamb_anc:%.8f'%float(hyper[\"lamb_anc\"]) , ',knn_auc: %.8f' % float(knn_score), ', mse_score: %.8f' % float(mse_score), file=f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
