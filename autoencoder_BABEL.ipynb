{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes = load_raw.load_raw_cell_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [],
   "source": [
    "scicar_data, joint_index, keep_cells_idx = load_raw.merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scica|r_data)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(adata, prop_clip=0.5):\n",
    "    assert isinstance(prop_clip, float) and 0.0 < prop_clip < 50.0\n",
    "    clip_low, clip_high = np.percentile(adata.X.flatten(), [prop_clip, 100 - prop_clip])\n",
    "    adata.X = np.clip(adata.X, clip_low, clip_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATAC_KWARGS = {\n",
    "    \"binarize\": True,\n",
    "    \"filter_gene_min_counts\": 5, \n",
    "    \"filter_gene_min_cells\": 5,\n",
    "    \"filter_gene_max_cells\": 0.1,\n",
    "}\n",
    "\n",
    "RNA_KWARGS = {\n",
    "    \"size_factor\": True, \n",
    "    \"log_trans\": True, \n",
    "    \"normalize\": True, \n",
    "    \"proportion\": None,\n",
    "    \"prop_clip\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    adata,\n",
    "    binarize=True,\n",
    "    filter_gene_min_counts=5, \n",
    "    filter_gene_min_cells=5,\n",
    "    filter_gene_max_cells=0.1,\n",
    "    size_factor=True, \n",
    "    log_trans=True, \n",
    "    normalize=True, \n",
    "    proportion=None,\n",
    "    prop_clip=0.5,\n",
    "):\n",
    "    #preprocess for RNA modality \n",
    "    #(log tranform normalize and scale to zero mena unit variance)\n",
    "    nm.normalize_count_table(adata, proportion=proportion)\n",
    "    clip(adata, prop_clip=prop_clip)\n",
    "    \n",
    "    #preprocess for ATAC modality (binarize and filter)\n",
    "    nm.binarize(adata, obsm=\"mode2\", obs=\"mode2_obs\", var=\"mode2_var\")\n",
    "    nm.filter_adata(\n",
    "        adata, \n",
    "        filter_gene_min_counts=5, \n",
    "        filter_gene_min_cells=5,\n",
    "        filter_gene_max_cells=0.1,\n",
    "        obsm=\"mode2\",\n",
    "        obs=\"mode2_obs\",\n",
    "        var=\"mode2_var\",\n",
    "    )\n",
    "    \n",
    "    if not isinstance(adata.X, scipy.sparse.csr_matrix):\n",
    "        adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "        \n",
    "    #figure out the obs and var info for adata\n",
    "    adata.uns[\"mode2_obs\"] = np.array(adata.uns[\"mode2_obs\"][0])\n",
    "    adata.uns[\"mode2_var\"] = np.array(adata.uns[\"mode2_var\"][0])\n",
    "    adata.uns = {\"mode2_obs\": adata.uns[\"mode2_obs\"], \"mode2_var\": adata.uns[\"mode2_var\"]}\n",
    "    \n",
    "    dim_mode1 = adata.X.shape[1]\n",
    "    dim_mode2 = adata.obsm[\"mode2\"].shape[1]\n",
    "    return adata, dim_mode1, dim_mode2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scicar_data, dim_rna, dim_atac = preprocess(scicar_data, **RNA_KWARGS, **ATAC_KWARGS)\n",
    "train_data, test_data = load_raw.train_test_split(scicar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":200,\n",
    "    \"dim_rna\":dim_rna,\n",
    "    \"dim_atac\":dim_atac,\n",
    "    \"n_latent\":32,\n",
    "    \"layer_sizes\":[64],\n",
    "    \"train_batch\":128,\n",
    "    \"test_batch\": 512,\n",
    "    \"lr\": 1e-3,\n",
    "    \"clip_grad\": 1,\n",
    "    \"weightDirName\": './checkpoint/',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "    def __init__(self, adata):\n",
    "        self.rna_data, self.atac_data = self._load_merge_data(adata)\n",
    "\n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.atac_data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        rna_sample = self.rna_data.values[idx]\n",
    "        atac_sample = self.atac_data.values[idx]\n",
    "        #return a tensor that for a single observation\n",
    "        return [torch.from_numpy(rna_sample).float(), torch.from_numpy(atac_sample).float()]\n",
    "  \n",
    "    def _load_merge_data(self, adata):\n",
    "        rna_df = pd.DataFrame(\n",
    "            data = adata.X.toarray(), \n",
    "            index = np.array(adata.obs.index), \n",
    "            columns = np.array(adata.var.index),\n",
    "        )\n",
    "        atac_df = pd.DataFrame(\n",
    "            data = adata.obsm[\"mode2\"].toarray(),\n",
    "            index = np.array(adata.uns[\"mode2_obs\"]),\n",
    "            columns = np.array(adata.uns[\"mode2_var\"]),\n",
    "        )\n",
    "        return rna_df, atac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models (BABEL) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(nn.Module):\n",
    "    \n",
    "    def __init__(self, min_val=1e-5, max_val=1e6):\n",
    "        super(Exp, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(\n",
    "            torch.exp(x),\n",
    "            min=self.min_val,\n",
    "            max=self.max_val,\n",
    "        )\n",
    "    \n",
    "class ClippedSoftplus(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta=1, threshold=20, min_val=1e-4, max_val=1e3):\n",
    "        super(ClippedSoftplus, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(\n",
    "            F.softplus(x, self.beta, self.threshold),\n",
    "            min=self.min_val,\n",
    "            max=self.max_val,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input, n_latent, layer_sizes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_input] + layer_sizes + [n_latent]\n",
    "        self.encoder_layers = []\n",
    "        \n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx + 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.encoder_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx + 1])\n",
    "            self.encoder_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.encoder_layers.append(act1)\n",
    "            \n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_output, n_latent, layer_sizes, final_activation=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_output = n_output\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_output] + layer_sizes + [n_latent]\n",
    "        self.decoder1_layers = []\n",
    "        for idx in range(len(self.layer_sizes) - 1, 1,  -1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx - 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.decoder1_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx - 1])\n",
    "            self.decoder1_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.decoder1_layers.append(act1)\n",
    "        self.decoder1 = nn.Sequential(*self.decoder1_layers)\n",
    "        \n",
    "        self.n_inter = self.layer_sizes[len(self.layer_sizes)-2]\n",
    "        self.decoder21 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder21.weight)\n",
    "        self.decoder22 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder22.weight)\n",
    "        self.decoder23 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder23.weight)\n",
    "        \n",
    "        self.final_activations = nn.ModuleDict()\n",
    "        if final_activation is not None:\n",
    "            if isinstance(final_activation, list) or isinstance(final_activation, tuple):\n",
    "                for i, act in enumerate(final_activation):\n",
    "                    self.final_activations[f\"act{i+1}\"] = act\n",
    "            elif isinstance(final_activation, nn.Module):\n",
    "                self.final_activations[\"act1\"] = final_activation\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized type for final_activation: {type(final_activation)}\"\n",
    "                )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.decoder1(x)\n",
    "            \n",
    "        retval1 = self.decoder21(x)\n",
    "        if \"act1\" in self.final_activations.keys():\n",
    "            retval1 = self.final_activations[\"act1\"](retval1)\n",
    "            \n",
    "        retval2 = self.decoder22(x)\n",
    "        if \"act2\" in self.final_activations.keys():\n",
    "            retval2 = self.final_activations[\"act2\"](retval2)\n",
    "            \n",
    "        retval3 = self.decoder23(x)\n",
    "        if \"act3\" in self.final_activations.keys():\n",
    "            retval3 = self.final_activations[\"act3\"](retval3)\n",
    "                \n",
    "        return retval1, retval2, retval3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSplicedAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim1, \n",
    "        input_dim2, \n",
    "        n_latent, \n",
    "        layer_sizes, \n",
    "        final_activation1, \n",
    "        final_activation2, \n",
    "        seed=182822,\n",
    "    ):\n",
    "        super(NaiveSplicedAutoencoder, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_outputs1 = (\n",
    "            len(final_activation1) \n",
    "            if isinstance(final_activation1, (list, set, tuple)) \n",
    "            else 1\n",
    "        )\n",
    "        self.num_outputs2 = (\n",
    "            len(final_activation2) \n",
    "            if isinstance(final_activation2, (list, set, tuple)) \n",
    "            else 1\n",
    "        )\n",
    "        \n",
    "        self.encoder1 = Encoder(n_input=input_dim1, n_latent=n_latent, layer_sizes=layer_sizes)\n",
    "        self.encoder2 = Encoder(n_input=input_dim2, n_latent=n_latent, layer_sizes=layer_sizes)\n",
    "        self.decoder1 = Decoder(\n",
    "            n_output=input_dim1,\n",
    "            n_latent=n_latent,\n",
    "            layer_sizes=layer_sizes,\n",
    "            final_activation=final_activation1,\n",
    "        )\n",
    "        self.decoder2 = Decoder(\n",
    "            n_output=input_dim2,\n",
    "            n_latent=n_latent,\n",
    "            layer_sizes=layer_sizes,\n",
    "            final_activation=final_activation2,\n",
    "        )\n",
    "    \n",
    "    def _combine_output_and_encoded(self, decoded, encoded, num_outputs):\n",
    "        if num_outputs > 0:\n",
    "            retval = *decoded, encoded\n",
    "        else:\n",
    "            if isinstance(decoded, (tuple, list)):\n",
    "                decoded = decoded[0]\n",
    "            retval = decoded, encoded\n",
    "            \n",
    "        return retval\n",
    "        \n",
    "    def forward_single(self, x, size_factors=None, in_domain=1, out_domain=1):\n",
    "        encoder = self.encoder1 if in_domain == 1 else self.encoder2\n",
    "        decoder = self.decoder1 if out_domain == 1 else self.decoder2\n",
    "        num_non_latent_output = self.num_ouptuts1 if out_domain == 1 else self.num_outputs2\n",
    "        encoded = encoder(x)\n",
    "        decoded = decoder(encoded)\n",
    "        return self._combine_output_and_encoded(decoded, encoded, num_non_latent_output)\n",
    "    \n",
    "    def forward(self, x, size_factors=None):\n",
    "        assert isinstance(x, (tuple, list))\n",
    "        assert len(x) == 2\n",
    "        encoded1 = self.encoder1(x[0])\n",
    "        encoded2 = self.encoder2(x[1])\n",
    "        \n",
    "        decoded11 = self.decoder1(encoded1)\n",
    "        retval11 = self._combine_output_and_encoded(\n",
    "            decoded11, encoded1, self.num_outputs1\n",
    "        )\n",
    "        \n",
    "        decoded12 = self.decoder2(encoded1)\n",
    "        retval12 = self._combine_output_and_encoded(\n",
    "            decoded12, encoded1, self.num_outputs2\n",
    "        )\n",
    "        \n",
    "        decoded21 = self.decoder1(encoded2)\n",
    "        retval21 = self._combine_output_and_encoded(\n",
    "            decoded21, encoded2, self.num_outputs1\n",
    "        )\n",
    "        \n",
    "        decoded22 = self.decoder2(encoded2)\n",
    "        retval22 = self._combine_output_and_encoded(\n",
    "            decoded22, encoded2, self.num_outputs2\n",
    "        )\n",
    "        \n",
    "        return retval11, retval12, retval21, retval22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(nn.BCELoss):\n",
    "    def forward(self, x, target):\n",
    "        x_input = x[0]\n",
    "        return nn.functional.binary_cross_entropy(\n",
    "            x_input, target, weight=self.weight, reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeBinomialLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale_factor=1.0,\n",
    "        eps=1e-10,\n",
    "        l1_lambda=0.0,\n",
    "        mean=True,\n",
    "    ):\n",
    "        super(NegativeBinomialLoss, self).__init__()\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.loss = self.get_nb_loss(\n",
    "            scale_factor=scale_factor, \n",
    "            eps=eps,\n",
    "            mean=mean,\n",
    "            debug=True,\n",
    "        )\n",
    "        self.eps = eps\n",
    "    def forward(self, preds, target):\n",
    "        pred, theta = preds[:2]\n",
    "        l = self.loss(\n",
    "            pred=pred,\n",
    "            theta=theta,\n",
    "            truth=target,\n",
    "        )\n",
    "            \n",
    "        #add l1 regularization term\n",
    "        params = pred[:-1]\n",
    "        l += self.l1_lambda * torch.abs(params).sum()\n",
    "        return l\n",
    "        \n",
    "    def get_nb_loss(\n",
    "            self,\n",
    "            scale_factor=1.0,\n",
    "            eps=1e-10,\n",
    "            mean=True,\n",
    "            debug=False,\n",
    "        ):\n",
    "            def loss(pred, theta, truth):\n",
    "                y_true = truth\n",
    "                y_hat = pred * scale_factor\n",
    "                \n",
    "                theta = torch.clamp(theta, max=1e6)\n",
    "                if debug:\n",
    "                    assert not torch.isnan(y_hat).any(), y_hat\n",
    "                    assert not torch.isinf(y_hat).any(), y_hat\n",
    "                    assert not (y_hat < 0).any()\n",
    "                    assert not (theta < 0).any()\n",
    "                    \n",
    "                t1 = (\n",
    "                    -torch.lgamma(y_true + theta + eps)\n",
    "                    +torch.lgamma(y_true + 1.0)\n",
    "                    +torch.lgamma(theta + eps)\n",
    "                )\n",
    "                \n",
    "                t2 = -theta * (torch.log(theta + eps) - torch.log(theta + y_hat)) - (\n",
    "                    y_true * (torch.log(y_hat + eps) - torch.log(theta + y_hat))\n",
    "                )\n",
    "                \n",
    "                t2_alter = (\n",
    "                    (theta + y_true) * (torch.log1p(y_hat/(theta + eps))) + \n",
    "                    y_true * (torch.log(theta + eps) - torch.log(y_hat + eps))\n",
    "                )\n",
    "                \n",
    "                if debug:\n",
    "                    assert not torch.isnan(t1).any(), t1\n",
    "                    assert not torch.isinf(t1).any(), (t1, torch.sum(torch.isinf(t1)))\n",
    "                    assert not torch.isnan(t2).any(), t2\n",
    "                    assert not torch.isinf(t2).any(), t2\n",
    "                    assert not torch.isnan(t2_alter).any(), t2_alter\n",
    "                    assert not torch.isinf(t2_alter).any(), t2_alter\n",
    "                    \n",
    "                retval = t1 + t2_alter\n",
    "                \n",
    "                if debug:\n",
    "                    assert not torch.isnan(retval).any(), retval\n",
    "                    assert not torch.isinf(retval).any(), retval\n",
    "                    \n",
    "                return torch.mean(retval) if mean else retval\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss1=NegativeBinomialLoss,\n",
    "        loss2=BCELoss,\n",
    "        loss2_weight=1.33, \n",
    "    ):\n",
    "        super(QuadLoss, self).__init__()\n",
    "        self.loss1 = loss1()\n",
    "        self.loss2 = loss2()\n",
    "        self.beta = loss2_weight\n",
    "        \n",
    "    def get_component_losses(self, preds, target):\n",
    "        \n",
    "        #recover prediction\n",
    "        pred11, pred12, pred21, pred22 = preds\n",
    "        target1, target2 = target\n",
    "        \n",
    "        loss11 = self.loss1(pred11, target1)\n",
    "        loss12 = self.loss2(pred12, target2)\n",
    "        loss21 = self.loss1(pred21, target1)\n",
    "        loss22 = self.loss2(pred22, target2)\n",
    "        \n",
    "        return loss11, loss12, loss21, loss22\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        loss11, loss12, loss21, loss22 = self.get_component_losses(preds, target)\n",
    "        loss = loss11 + loss21 + self.beta * (loss12 + loss22)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "def get_data_loaders(train_data, test_data):\n",
    "    train_set = Merge_Dataset(train_data)\n",
    "    test_set = Merge_Dataset(test_data)\n",
    "    #load data loader\n",
    "    train_loader = DataLoader(train_set, batch_size=hyper[\"train_batch\"], drop_last=False, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=hyper[\"test_batch\"], drop_last=False, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_criteria(rna_inputs, atac_inputs, rna_outputs, atac_outputs, proportion_neighbors=0.1, n_svd=100):\n",
    "    n_svd = min([n_svd, min(rna_inputs.shape)-1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors*rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(rna_inputs).kneighbors(rna_inputs)\n",
    "    )\n",
    "    _, indices_pred = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(rna_outputs).kneighbors(atac_outputs)\n",
    "    )\n",
    "    neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "    for i in range(rna_inputs.shape[0]):\n",
    "        _, pred_matches, true_matches = np.intersect1d(\n",
    "            indices_pred[i], indices_true[i], return_indices=True\n",
    "        )\n",
    "        neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "        neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "    neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * rna_inputs.shape[0])\n",
    "    area_under_curve = np.mean(neighbors_match_curve)\n",
    "    return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "babel_model = NaiveSplicedAutoencoder(\n",
    "        input_dim1=hyper[\"dim_rna\"], \n",
    "        input_dim2=hyper[\"dim_atac\"], \n",
    "        n_latent=hyper[\"n_latent\"], \n",
    "        layer_sizes=hyper[\"layer_sizes\"], \n",
    "        final_activation1=[Exp(), ClippedSoftplus()], \n",
    "        final_activation2=nn.Sigmoid(), \n",
    "        seed=182822,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = QuadLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, test_adata, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        rna_inputs = Variable(torch.from_numpy(test_adata.X.toarray()).float())\n",
    "        atac_inputs = Variable(torch.from_numpy(test_adata.obsm[\"mode2\"].toarray()).float())\n",
    "        rna_inputs = rna_inputs.to(device)\n",
    "        atac_inputs = atac_inputs.to(device)\n",
    "        inputs_list = [rna_inputs, atac_inputs]\n",
    "        retval11, retval12, retval21, retval22 = model(inputs_list)\n",
    "        z_rna = retval11[-1]\n",
    "        z_atac = retval22[-1]\n",
    "        \n",
    "    test_adata.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "    test_adata.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "    knn_score, mse_score = metrics.knn_auc(test_adata), metrics.mse(test_adata)\n",
    "    return knn_score, mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def main():\n",
    "    \n",
    "    #load training data and testing data\n",
    "    train_loader, test_loader = get_data_loaders(train_data, test_data)\n",
    "    \n",
    "    #load basic model\n",
    "    babel_model = NaiveSplicedAutoencoder(\n",
    "        input_dim1=hyper[\"dim_rna\"], \n",
    "        input_dim2=hyper[\"dim_atac\"], \n",
    "        n_latent=hyper[\"n_latent\"], \n",
    "        layer_sizes=hyper[\"layer_sizes\"], \n",
    "        final_activation1=[Exp(), ClippedSoftplus()], \n",
    "        final_activation2=nn.Sigmoid(), \n",
    "        seed=182822,  \n",
    "    )\n",
    "    \n",
    "    #set up losses\n",
    "    criterion = QuadLoss()\n",
    "    \n",
    "    #set up optimizer\n",
    "    babel_opt = optim.Adam(list(babel_model.parameters()), lr=hyper[\"lr\"])\n",
    "    \n",
    "    #set up device \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    babel_model.to(device)\n",
    "\n",
    "    #training step\n",
    "    for epoch in range(hyper[\"nEpochs\"]):\n",
    "        train_losses = []\n",
    "        for idx, (rna_inputs, atac_inputs) in enumerate(train_loader):\n",
    "            babel_opt.zero_grad()\n",
    "\n",
    "            rna_inputs = Variable(rna_inputs).to(device)\n",
    "            atac_inputs = Variable(atac_inputs).to(device)\n",
    "            inputs_list = [rna_inputs, atac_inputs]\n",
    "            retval11, retval12, retval21, retval22 = babel_model(inputs_list)\n",
    "            preds = (retval11, retval12, retval21, retval22)\n",
    "\n",
    "            train_loss = criterion(preds, inputs_list)\n",
    "            train_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(babel_model.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "            babel_opt.step()\n",
    "\n",
    "            train_losses.append(train_loss.item())\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(np.mean(train_losses)))\n",
    "        \n",
    "        #evaluating step\n",
    "        with torch.no_grad():\n",
    "            babel_model.eval()\n",
    "            knn_acc = []\n",
    "            #mse_acc = []\n",
    "            for idx, samples in enumerate(test_loader):\n",
    "                rna_inputs = samples[0].float()\n",
    "                atac_inputs = samples[1].float()\n",
    "                rna_inputs = rna_inputs.to(device)\n",
    "                atac_inputs = atac_inputs.to(device)\n",
    "\n",
    "                inputs_list = [rna_inputs, atac_inputs]\n",
    "\n",
    "                retval11, retval12, retval21, retval22 = babel_model(inputs_list)\n",
    "                output_rna = retval11[-1]\n",
    "                output_atac = retval22[-1]\n",
    "                knn_acc.append(knn_criteria(rna_inputs.cpu().detach(), atac_inputs.cpu().detach(), \n",
    "                                            output_rna.cpu().detach(), output_atac.cpu().detach()))\n",
    "            avg_knn_auc = np.mean(knn_acc)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", acc: \" + str(avg_knn_auc))\n",
    "    test_knn_score, test_mse_score = model_eval(babel_model, test_data, device)\n",
    "    print(\"Final knn_auc: \" + str(test_knn_score))\n",
    "    print(\"Final mse: \" + str(test_mse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 2.3176811016522922\n",
      "Epoch: 0, acc: 0.004492031015350153\n",
      "Epoch: 5, train loss: -0.5477063988263791\n",
      "Epoch: 5, acc: 0.016193275388781167\n",
      "Epoch: 10, train loss: -0.5387228320424373\n",
      "Epoch: 10, acc: 0.012265943826860706\n",
      "Epoch: 15, train loss: -0.5571198211266444\n",
      "Epoch: 15, acc: 0.006473791801125663\n",
      "Epoch: 20, train loss: -0.5024110606083503\n",
      "Epoch: 20, acc: 0.024740185805259312\n",
      "Epoch: 25, train loss: -0.5904764384031296\n",
      "Epoch: 25, acc: 0.0025085418796699757\n",
      "Epoch: 30, train loss: -0.884712984928718\n",
      "Epoch: 30, acc: 0.013615831136346216\n",
      "Epoch: 35, train loss: -0.8799001345267663\n",
      "Epoch: 35, acc: 0.008300753963306826\n",
      "Epoch: 40, train loss: -0.881984178836529\n",
      "Epoch: 40, acc: 0.01383186651714117\n",
      "Epoch: 45, train loss: -0.8145969991500561\n",
      "Epoch: 45, acc: 0.0062356478975144\n",
      "Epoch: 50, train loss: -1.0927412876716027\n",
      "Epoch: 50, acc: 0.0034873609848547395\n",
      "Epoch: 55, train loss: -1.1579935321441064\n",
      "Epoch: 55, acc: 0.01389311983117883\n",
      "Epoch: 60, train loss: -1.1833061575889587\n",
      "Epoch: 60, acc: 0.023548486928243382\n",
      "Epoch: 65, train loss: -1.1025042579724238\n",
      "Epoch: 65, acc: 0.006996603275850698\n",
      "Epoch: 70, train loss: -1.0381561448940864\n",
      "Epoch: 70, acc: 0.0030060471277516443\n",
      "Epoch: 75, train loss: -1.2557436869694636\n",
      "Epoch: 75, acc: 0.01230453926843382\n",
      "Epoch: 80, train loss: -1.2207351602040803\n",
      "Epoch: 80, acc: 0.011933252393051437\n",
      "Epoch: 85, train loss: -1.1941896493618305\n",
      "Epoch: 85, acc: 0.012646300859301135\n",
      "Epoch: 90, train loss: -1.2489032195164607\n",
      "Epoch: 90, acc: 0.012672666340177855\n",
      "Epoch: 95, train loss: -1.2381775333331182\n",
      "Epoch: 95, acc: 0.06911625698608027\n",
      "Epoch: 100, train loss: -1.2651642194161048\n",
      "Epoch: 100, acc: 0.007078047414505906\n",
      "Epoch: 105, train loss: -1.2645581639730012\n",
      "Epoch: 105, acc: 0.012471256052448294\n",
      "Epoch: 110, train loss: -1.3062632129742549\n",
      "Epoch: 110, acc: 0.0023053503254598638\n",
      "Epoch: 115, train loss: -1.1588478844899397\n",
      "Epoch: 115, acc: 0.006095375263264829\n",
      "Epoch: 120, train loss: -1.2776695352334242\n",
      "Epoch: 120, acc: 0.002152494977217294\n",
      "Epoch: 125, train loss: -1.2168130920483515\n",
      "Epoch: 125, acc: 0.006234717658797532\n",
      "Epoch: 130, train loss: -1.2285530337920556\n",
      "Epoch: 130, acc: 0.005389886773723857\n",
      "Epoch: 135, train loss: -1.2554898262023926\n",
      "Epoch: 135, acc: 0.006677941759120232\n",
      "Epoch: 140, train loss: -1.33929619880823\n",
      "Epoch: 140, acc: 0.002272123365266051\n",
      "Epoch: 145, train loss: -1.3350880925471966\n",
      "Epoch: 145, acc: 0.003913397260742957\n",
      "Epoch: 150, train loss: -1.2995045643586378\n",
      "Epoch: 150, acc: 0.0023385291313854618\n",
      "Epoch: 155, train loss: -1.3454277102763836\n",
      "Epoch: 155, acc: 0.011721547824526066\n",
      "Epoch: 160, train loss: -1.3509179995610163\n",
      "Epoch: 160, acc: 0.008496566411033817\n",
      "Epoch: 165, train loss: -1.419174272280473\n",
      "Epoch: 165, acc: 0.012816728831729922\n",
      "Epoch: 170, train loss: -1.2051048232958868\n",
      "Epoch: 170, acc: 0.012776761041726786\n",
      "Epoch: 175, train loss: -1.3233215763018682\n",
      "Epoch: 175, acc: 0.009421152237434166\n",
      "Epoch: 180, train loss: -1.4204822824551508\n",
      "Epoch: 180, acc: 0.008609622101826631\n",
      "Epoch: 185, train loss: -1.4442991476792555\n",
      "Epoch: 185, acc: 0.004001101137828921\n",
      "Epoch: 190, train loss: -1.39383098253837\n",
      "Epoch: 190, acc: 0.1382318706397059\n",
      "Epoch: 195, train loss: -1.4868177725718572\n",
      "Epoch: 195, acc: 0.006030526997684345\n",
      "Final knn_auc: 0.038756288473742885\n",
      "Final mse: 0.9415735\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0958825348422088\n",
      "0.9997422\n",
      "0.09558056510033025\n",
      "1.0048813\n"
     ]
    }
   ],
   "source": [
    "#test knn_auc plateau at around 0.09, seems that training starts to overfit\n",
    "test_knn_score, test_mse_score = model_eval(test_data)\n",
    "print(test_knn_score)\n",
    "print(test_mse_score)\n",
    "train_knn_score, train_mse_score = model_eval(train_data)\n",
    "print(train_knn_score)\n",
    "print(train_mse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log the metrics\n",
    "path = \"{}Max_iter_{}lamb_anc_{}metrics.txt\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]))\n",
    "'''torch.save({\n",
    "    \"num_iter\": hyper[\"nEpochs\"],\n",
    "    \"lamb_anc\": hyper[\"lamb_anc\"],\n",
    "    'knn_auc': knn_score,\n",
    "    'mse': mse_score,\n",
    "}, path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'a') as f:\n",
    "        print('nEpoch: ', hyper[\"nEpochs\"], 'lamb_anc:%.8f'%float(hyper[\"lamb_anc\"]) , ',knn_auc: %.8f' % float(knn_score), ', mse_score: %.8f' % float(mse_score), file=f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
