{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes = load_raw.load_raw_cell_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [],
   "source": [
    "scicar_data, joint_index, keep_cells_idx = load_raw.merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scica|r_data)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(adata, prop_clip=0.5):\n",
    "    assert isinstance(prop_clip, float) and 0.0 < prop_clip < 50.0\n",
    "    clip_low, clip_high = np.percentile(adata.X.flatten(), [prop_clip, 100 - prop_clip])\n",
    "    adata.X = np.clip(adata.X, clip_low, clip_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATAC_KWARGS = {\n",
    "    \"binarize\": True,\n",
    "    \"filter_gene_min_counts\": 5, \n",
    "    \"filter_gene_min_cells\": 5,\n",
    "    \"filter_gene_max_cells\": 0.1,\n",
    "}\n",
    "\n",
    "RNA_KWARGS = {\n",
    "    \"size_factor\": True, \n",
    "    \"log_trans\": True, \n",
    "    \"normalize\": True, \n",
    "    \"proportion\": None,\n",
    "    \"prop_clip\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    adata,\n",
    "    binarize=True,\n",
    "    filter_gene_min_counts=5, \n",
    "    filter_gene_min_cells=5,\n",
    "    filter_gene_max_cells=0.1,\n",
    "    size_factor=True, \n",
    "    log_trans=True, \n",
    "    normalize=True, \n",
    "    proportion=None,\n",
    "    prop_clip=0.5,\n",
    "):\n",
    "    #preprocess for RNA modality \n",
    "    #(log tranform normalize and scale to zero mena unit variance)\n",
    "    nm.normalize_count_table(adata, proportion=proportion)\n",
    "    clip(adata, prop_clip=prop_clip)\n",
    "    \n",
    "    #preprocess for ATAC modality (binarize and filter)\n",
    "    nm.binarize(adata, obsm=\"mode2\", obs=\"mode2_obs\", var=\"mode2_var\")\n",
    "    nm.filter_adata(\n",
    "        adata, \n",
    "        filter_gene_min_counts=5, \n",
    "        filter_gene_min_cells=5,\n",
    "        filter_gene_max_cells=0.1,\n",
    "        obsm=\"mode2\",\n",
    "        obs=\"mode2_obs\",\n",
    "        var=\"mode2_var\",\n",
    "    )\n",
    "    \n",
    "    if not isinstance(adata.X, scipy.sparse.csr_matrix):\n",
    "        adata.X = scipy.sparse.csr_matrix(adata.X)\n",
    "        \n",
    "    #figure out the obs and var info for adata\n",
    "    adata.uns[\"mode2_obs\"] = np.array(adata.uns[\"mode2_obs\"][0])\n",
    "    adata.uns[\"mode2_var\"] = np.array(adata.uns[\"mode2_var\"][0])\n",
    "    adata.uns = {\"mode2_obs\": adata.uns[\"mode2_obs\"], \"mode2_var\": adata.uns[\"mode2_var\"]}\n",
    "    \n",
    "    dim_mode1 = adata.X.shape[1]\n",
    "    dim_mode2 = adata.obsm[\"mode2\"].shape[1]\n",
    "    return adata, dim_mode1, dim_mode2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60550\n",
      "57916\n"
     ]
    }
   ],
   "source": [
    "scicar_data, dim_rna, dim_atac = preprocess(scicar_data, **RNA_KWARGS, **ATAC_KWARGS)\n",
    "train_data, test_data = load_raw.train_test_split(scicar_data)\n",
    "print(dim_rna)\n",
    "print(dim_atac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":55,\n",
    "    \"dim_rna\":dim_rna,\n",
    "    \"dim_atac\":dim_atac,\n",
    "    \"n_latent\":32,\n",
    "    \"layer_sizes\":[64],\n",
    "    \"add_hinge\":True,\n",
    "    \"lamb_match\":1,\n",
    "    \"lamb_nn\":0.4,\n",
    "    \"train_batch\":256,\n",
    "    \"test_batch\": 512,\n",
    "    \"lr\": 1e-3,\n",
    "    \"clip_grad\": 1,\n",
    "    \"weightDirName\": './checkpoint/',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "    def __init__(self, adata):\n",
    "        self.rna_data, self.atac_data = self._load_merge_data(adata)\n",
    "\n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.atac_data)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        rna_sample = self.rna_data.values[idx]\n",
    "        atac_sample = self.atac_data.values[idx]\n",
    "        #return a tensor that for a single observation\n",
    "        return [torch.from_numpy(rna_sample).float(), torch.from_numpy(atac_sample).float()]\n",
    "  \n",
    "    def _load_merge_data(self, adata):\n",
    "        rna_df = pd.DataFrame(\n",
    "            data = adata.X.toarray(), \n",
    "            index = np.array(adata.obs.index), \n",
    "            columns = np.array(adata.var.index),\n",
    "        )\n",
    "        atac_df = pd.DataFrame(\n",
    "            data = adata.obsm[\"mode2\"].toarray(),\n",
    "            index = np.array(adata.uns[\"mode2_obs\"]),\n",
    "            columns = np.array(adata.uns[\"mode2_var\"]),\n",
    "        )\n",
    "        return rna_df, atac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models (BABEL) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exp(nn.Module):\n",
    "    \n",
    "    def __init__(self, min_val=1e-5, max_val=1e6):\n",
    "        super(Exp, self).__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(\n",
    "            torch.exp(x),\n",
    "            min=self.min_val,\n",
    "            max=self.max_val,\n",
    "        )\n",
    "    \n",
    "class ClippedSoftplus(nn.Module):\n",
    "    \n",
    "    def __init__(self, beta=1, threshold=20, min_val=1e-4, max_val=1e3):\n",
    "        super(ClippedSoftplus, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.threshold = threshold\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.clamp(\n",
    "            F.softplus(x, self.beta, self.threshold),\n",
    "            min=self.min_val,\n",
    "            max=self.max_val,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input, n_latent, layer_sizes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_input] + layer_sizes + [n_latent]\n",
    "        self.encoder_layers = []\n",
    "        \n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx + 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.encoder_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx + 1])\n",
    "            self.encoder_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.encoder_layers.append(act1)\n",
    "            \n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_output, n_latent, layer_sizes, final_activation=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_output = n_output\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_output] + layer_sizes + [n_latent]\n",
    "        self.decoder1_layers = []\n",
    "        for idx in range(len(self.layer_sizes) - 1, 1,  -1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx - 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.decoder1_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx - 1])\n",
    "            self.decoder1_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.decoder1_layers.append(act1)\n",
    "        self.decoder1 = nn.Sequential(*self.decoder1_layers)\n",
    "        \n",
    "        self.n_inter = self.layer_sizes[len(self.layer_sizes)-2]\n",
    "        self.decoder21 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder21.weight)\n",
    "        self.decoder22 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder22.weight)\n",
    "        self.decoder23 = nn.Linear(self.n_inter, self.n_output)\n",
    "        nn.init.xavier_uniform_(self.decoder23.weight)\n",
    "        \n",
    "        self.final_activations = nn.ModuleDict()\n",
    "        if final_activation is not None:\n",
    "            if isinstance(final_activation, list) or isinstance(final_activation, tuple):\n",
    "                for i, act in enumerate(final_activation):\n",
    "                    self.final_activations[f\"act{i+1}\"] = act\n",
    "            elif isinstance(final_activation, nn.Module):\n",
    "                self.final_activations[\"act1\"] = final_activation\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized type for final_activation: {type(final_activation)}\"\n",
    "                )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.decoder1(x)\n",
    "            \n",
    "        retval1 = self.decoder21(x)\n",
    "        if \"act1\" in self.final_activations.keys():\n",
    "            retval1 = self.final_activations[\"act1\"](retval1)\n",
    "            \n",
    "        retval2 = self.decoder22(x)\n",
    "        if \"act2\" in self.final_activations.keys():\n",
    "            retval2 = self.final_activations[\"act2\"](retval2)\n",
    "            \n",
    "        retval3 = self.decoder23(x)\n",
    "        if \"act3\" in self.final_activations.keys():\n",
    "            retval3 = self.final_activations[\"act3\"](retval3)\n",
    "                \n",
    "        return retval1, retval2, retval3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSplicedAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim1, \n",
    "        input_dim2, \n",
    "        n_latent, \n",
    "        layer_sizes, \n",
    "        final_activation1, \n",
    "        final_activation2, \n",
    "        seed=182822,\n",
    "    ):\n",
    "        super(NaiveSplicedAutoencoder, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.input_dim1 = input_dim1\n",
    "        self.input_dim2 = input_dim2\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_outputs1 = (\n",
    "            len(final_activation1) \n",
    "            if isinstance(final_activation1, (list, set, tuple)) \n",
    "            else 1\n",
    "        )\n",
    "        self.num_outputs2 = (\n",
    "            len(final_activation2) \n",
    "            if isinstance(final_activation2, (list, set, tuple)) \n",
    "            else 1\n",
    "        )\n",
    "        \n",
    "        self.encoder1 = Encoder(n_input=input_dim1, n_latent=n_latent, layer_sizes=layer_sizes)\n",
    "        self.encoder2 = Encoder(n_input=input_dim2, n_latent=n_latent, layer_sizes=layer_sizes)\n",
    "        self.decoder1 = Decoder(\n",
    "            n_output=input_dim1,\n",
    "            n_latent=n_latent,\n",
    "            layer_sizes=layer_sizes,\n",
    "            final_activation=final_activation1,\n",
    "        )\n",
    "        self.decoder2 = Decoder(\n",
    "            n_output=input_dim2,\n",
    "            n_latent=n_latent,\n",
    "            layer_sizes=layer_sizes,\n",
    "            final_activation=final_activation2,\n",
    "        )\n",
    "    \n",
    "    def _combine_output_and_encoded(self, decoded, encoded, num_outputs):\n",
    "        if num_outputs > 0:\n",
    "            retval = *decoded, encoded\n",
    "        else:\n",
    "            if isinstance(decoded, (tuple, list)):\n",
    "                decoded = decoded[0]\n",
    "            retval = decoded, encoded\n",
    "            \n",
    "        return retval\n",
    "        \n",
    "    def forward_single(self, x, size_factors=None, in_domain=1, out_domain=1):\n",
    "        encoder = self.encoder1 if in_domain == 1 else self.encoder2\n",
    "        decoder = self.decoder1 if out_domain == 1 else self.decoder2\n",
    "        num_non_latent_output = self.num_ouptuts1 if out_domain == 1 else self.num_outputs2\n",
    "        encoded = encoder(x)\n",
    "        decoded = decoder(encoded)\n",
    "        return self._combine_output_and_encoded(decoded, encoded, num_non_latent_output)\n",
    "    \n",
    "    def forward(self, x, size_factors=None):\n",
    "        assert isinstance(x, (tuple, list))\n",
    "        assert len(x) == 2\n",
    "        encoded1 = self.encoder1(x[0])\n",
    "        encoded2 = self.encoder2(x[1])\n",
    "        \n",
    "        decoded11 = self.decoder1(encoded1)\n",
    "        retval11 = self._combine_output_and_encoded(\n",
    "            decoded11, encoded1, self.num_outputs1\n",
    "        )\n",
    "        \n",
    "        decoded12 = self.decoder2(encoded1)\n",
    "        retval12 = self._combine_output_and_encoded(\n",
    "            decoded12, encoded1, self.num_outputs2\n",
    "        )\n",
    "        \n",
    "        decoded21 = self.decoder1(encoded2)\n",
    "        retval21 = self._combine_output_and_encoded(\n",
    "            decoded21, encoded2, self.num_outputs1\n",
    "        )\n",
    "        \n",
    "        decoded22 = self.decoder2(encoded2)\n",
    "        retval22 = self._combine_output_and_encoded(\n",
    "            decoded22, encoded2, self.num_outputs2\n",
    "        )\n",
    "        \n",
    "        return retval11, retval12, retval21, retval22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(nn.BCELoss):\n",
    "    def forward(self, x, target):\n",
    "        x_input = x[0]\n",
    "        return nn.functional.binary_cross_entropy(\n",
    "            x_input, target, weight=self.weight, reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeBinomialLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scale_factor=1.0,\n",
    "        eps=1e-10,\n",
    "        l1_lambda=0.0,\n",
    "        mean=True,\n",
    "    ):\n",
    "        super(NegativeBinomialLoss, self).__init__()\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.loss = self.get_nb_loss(\n",
    "            scale_factor=scale_factor, \n",
    "            eps=eps,\n",
    "            mean=mean,\n",
    "            debug=True,\n",
    "        )\n",
    "        self.eps = eps\n",
    "    def forward(self, preds, target):\n",
    "        pred, theta = preds[:2]\n",
    "        l = self.loss(\n",
    "            pred=pred,\n",
    "            theta=theta,\n",
    "            truth=target,\n",
    "        )\n",
    "            \n",
    "        #add l1 regularization term\n",
    "        params = pred[:-1]\n",
    "        l += self.l1_lambda * torch.abs(params).sum()\n",
    "        return l\n",
    "        \n",
    "    def get_nb_loss(\n",
    "            self,\n",
    "            scale_factor=1.0,\n",
    "            eps=1e-10,\n",
    "            mean=True,\n",
    "            debug=False,\n",
    "        ):\n",
    "            def loss(pred, theta, truth):\n",
    "                y_true = truth\n",
    "                y_hat = pred * scale_factor\n",
    "                \n",
    "                theta = torch.clamp(theta, max=1e6)\n",
    "                if debug:\n",
    "                    assert not torch.isnan(y_hat).any(), y_hat\n",
    "                    assert not torch.isinf(y_hat).any(), y_hat\n",
    "                    assert not (y_hat < 0).any()\n",
    "                    assert not (theta < 0).any()\n",
    "                    \n",
    "                t1 = (\n",
    "                    -torch.lgamma(y_true + theta + eps)\n",
    "                    +torch.lgamma(y_true + 1.0)\n",
    "                    +torch.lgamma(theta + eps)\n",
    "                )\n",
    "                \n",
    "                t2 = -theta * (torch.log(theta + eps) - torch.log(theta + y_hat)) - (\n",
    "                    y_true * (torch.log(y_hat + eps) - torch.log(theta + y_hat))\n",
    "                )\n",
    "                \n",
    "                t2_alter = (\n",
    "                    (theta + y_true) * (torch.log1p(y_hat/(theta + eps))) + \n",
    "                    y_true * (torch.log(theta + eps) - torch.log(y_hat + eps))\n",
    "                )\n",
    "                \n",
    "                if debug:\n",
    "                    assert not torch.isnan(t1).any(), t1\n",
    "                    assert not torch.isinf(t1).any(), (t1, torch.sum(torch.isinf(t1)))\n",
    "                    assert not torch.isnan(t2).any(), t2\n",
    "                    assert not torch.isinf(t2).any(), t2\n",
    "                    assert not torch.isnan(t2_alter).any(), t2_alter\n",
    "                    assert not torch.isinf(t2_alter).any(), t2_alter\n",
    "                    \n",
    "                retval = t1 + t2_alter\n",
    "                \n",
    "                if debug:\n",
    "                    assert not torch.isnan(retval).any(), retval\n",
    "                    assert not torch.isinf(retval).any(), retval\n",
    "                    \n",
    "                return torch.mean(retval) if mean else retval\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss1=NegativeBinomialLoss,\n",
    "        loss2=BCELoss,\n",
    "        loss2_weight=1.33, \n",
    "    ):\n",
    "        super(QuadLoss, self).__init__()\n",
    "        self.loss1 = loss1()\n",
    "        self.loss2 = loss2()\n",
    "        self.beta = loss2_weight\n",
    "        \n",
    "    def get_component_losses(self, preds, target):\n",
    "        \n",
    "        #recover prediction\n",
    "        pred11, pred12, pred21, pred22 = preds\n",
    "        target1, target2 = target\n",
    "        \n",
    "        loss11 = self.loss1(pred11, target1)\n",
    "        loss12 = self.loss2(pred12, target2)\n",
    "        loss21 = self.loss1(pred21, target1)\n",
    "        loss22 = self.loss2(pred22, target2)\n",
    "        \n",
    "        return loss11, loss12, loss21, loss22\n",
    "    \n",
    "    def forward(self, preds, target):\n",
    "        loss11, loss12, loss21, loss22 = self.get_component_losses(preds, target)\n",
    "        loss = loss11 + loss21 + self.beta * (loss12 + loss22)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureHingeLoss(nn.Module):\n",
    "    def __init__(self, margin, max_val, lamb_match, lamb_nn, device):\n",
    "        super(StructureHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.max_val = max_val\n",
    "        self.lamb_match = lamb_match\n",
    "        self.lamb_nn = lamb_nn\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, rna_outputs, atac_outputs, nn_indices):\n",
    "        #rna_outputs: n_batch x n_latent\n",
    "        #atac_outputs: n_batch x n_latent\n",
    "        assert rna_outputs.shape[0] == atac_outputs.shape[0]\n",
    "        assert rna_outputs.shape[1] == atac_outputs.shape[1]\n",
    "        n_batch = rna_outputs.shape[0]\n",
    "        \n",
    "        #calculated pairwise L2 distance\n",
    "        #dist_rna_atac[i][j]: the L2 distance between RNA embedding i\n",
    "        #and ATAC embedding j (n_batch x n_batch)\n",
    "        #constraint for ensuring every rna embedding is close to matched atac embedding\n",
    "        dist_rna_atac = torch.cdist(rna_outputs, atac_outputs, p=2)\n",
    "        match_labels = torch.eye(n_batch).to(self.device)\n",
    "        match_mask = match_labels > 0\n",
    "        pos_match_dist = torch.masked_select(dist_rna_atac, match_mask).view(n_batch, 1)\n",
    "        neg_match_dist = torch.masked_select(dist_rna_atac, ~match_mask).view(n_batch, -1)\n",
    "        \n",
    "        loss_match_rna = torch.clamp(self.margin + pos_match_dist - neg_match_dist, 0, self.max_val)\n",
    "        loss_match_rna = loss_match_rna.mean()\n",
    "        #print(f\"loss_match_rna: {loss_match_rna}\")\n",
    "        \n",
    "        #constraint for ensuring every atac embedding is close to matched rna embedding\n",
    "        dist_atac_rna = dist_rna_atac.t()\n",
    "        pos_match_dist = torch.masked_select(dist_atac_rna, match_mask).view(n_batch, 1)\n",
    "        neg_match_dist = torch.masked_select(dist_atac_rna, ~match_mask).view(n_batch, -1)\n",
    "        \n",
    "        loss_match_atac = torch.clamp(self.margin + pos_match_dist - neg_match_dist, 0, self.max_val)\n",
    "        loss_match_atac = loss_match_rna.mean()\n",
    "        #print(f\"loss_match_atac: {loss_match_atac}\")\n",
    "        \n",
    "        #constraint for ensuring that every RNA embedding is close to \n",
    "        #the neighboring RNA embeddings.\n",
    "        nn_masked = torch.zeros(n_batch, n_batch).to(self.device)\n",
    "        nn_masked.scatter_(1, nn_indices, 1.)\n",
    "        nn_masked = nn_masked > 0\n",
    "        \n",
    "        dist_rna_rna = torch.cdist(rna_outputs, rna_outputs, p=2)\n",
    "        \n",
    "        #pos_rna_nn_dist: n_batch x n_neighbor\n",
    "        pos_rna_nn_dist = torch.masked_select(dist_rna_rna, nn_masked).view(n_batch, -1)\n",
    "        neg_rna_nn_dist = torch.masked_select(dist_rna_rna, ~nn_masked).view(n_batch, -1)\n",
    "        rna_nn_loss = torch.clamp(self.margin + pos_rna_nn_dist[...,None] - neg_rna_nn_dist[..., None, :], 0, self.max_val)\n",
    "        rna_nn_loss = rna_nn_loss.mean()\n",
    "        #print(f\"rna_nn_loss: {rna_nn_loss}\")\n",
    "        \n",
    "        #constraint for ensuring that every ATAC embedding is close to \n",
    "        #the neighboring ATAC embeddings.\n",
    "        dist_atac_atac = torch.cdist(atac_outputs, atac_outputs, p=2)\n",
    "        #pos_rna_nn_dist: n_batch x n_neighbor\n",
    "        pos_atac_nn_dist = torch.masked_select(dist_atac_atac, nn_masked).view(n_batch, -1)\n",
    "        neg_atac_nn_dist = torch.masked_select(dist_atac_atac, ~nn_masked).view(n_batch, -1)\n",
    "        atac_nn_loss = torch.clamp(self.margin + pos_atac_nn_dist[...,None] - neg_atac_nn_dist[..., None, :], 0, self.max_val)\n",
    "        atac_nn_loss = atac_nn_loss.mean()\n",
    "        #print(f\"atac_nn_loss: {atac_nn_loss}\")\n",
    "        \n",
    "        loss = (self.lamb_match * loss_match_rna \n",
    "                + self.lamb_match * loss_match_atac\n",
    "                + self.lamb_nn * rna_nn_loss \n",
    "                + self.lamb_nn * atac_nn_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    n_svd = 100\n",
    "    proportion_neighbors = 0.1\n",
    "    \n",
    "    rna_inputs, atac_inputs = zip(*batch)\n",
    "    rna_inputs = torch.stack(rna_inputs)\n",
    "    atac_inputs = torch.stack(atac_inputs)\n",
    "    \n",
    "    n_svd = min([n_svd, min(rna_inputs.shape) - 1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors * rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "    )\n",
    "    \n",
    "    return rna_inputs, atac_inputs, torch.from_numpy(indices_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "def get_data_loaders(train_data, test_data):\n",
    "    train_set = Merge_Dataset(train_data)\n",
    "    test_set = Merge_Dataset(test_data)\n",
    "    #load data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_set, \n",
    "        batch_size=hyper[\"train_batch\"], \n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False, \n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, \n",
    "        batch_size=hyper[\"test_batch\"], \n",
    "        collate_fn=collate_fn,\n",
    "        drop_last=False, \n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_criteria(rna_inputs, atac_inputs, rna_outputs, atac_outputs, proportion_neighbors=0.1, n_svd=100):\n",
    "    n_svd = min([n_svd, min(rna_inputs.shape)-1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors*rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "    )\n",
    "    _, indices_pred = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(rna_outputs).kneighbors(atac_outputs)\n",
    "    )\n",
    "    neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "    for i in range(rna_inputs.shape[0]):\n",
    "        _, pred_matches, true_matches = np.intersect1d(\n",
    "            indices_pred[i], indices_true[i], return_indices=True\n",
    "        )\n",
    "        neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "        neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "    neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * rna_inputs.shape[0])\n",
    "    area_under_curve = np.mean(neighbors_match_curve)\n",
    "    return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, test_adata, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        rna_inputs = Variable(torch.from_numpy(test_adata.X.toarray()).float())\n",
    "        atac_inputs = Variable(torch.from_numpy(test_adata.obsm[\"mode2\"].toarray()).float())\n",
    "        rna_inputs = rna_inputs.to(device)\n",
    "        atac_inputs = atac_inputs.to(device)\n",
    "        inputs_list = [rna_inputs, atac_inputs]\n",
    "        retval11, retval12, retval21, retval22 = model(inputs_list)\n",
    "        z_rna = retval11[-1]\n",
    "        z_atac = retval22[-1]\n",
    "        \n",
    "    test_adata.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "    test_adata.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "    knn_score, mse_score = metrics.knn_auc(test_adata), metrics.mse(test_adata)\n",
    "    return knn_score, mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def main():\n",
    "    \n",
    "    #load training data and testing data\n",
    "    train_loader, test_loader = get_data_loaders(train_data, test_data)\n",
    "    \n",
    "    #load basic model\n",
    "    babel_model = NaiveSplicedAutoencoder(\n",
    "        input_dim1=hyper[\"dim_rna\"], \n",
    "        input_dim2=hyper[\"dim_atac\"], \n",
    "        n_latent=hyper[\"n_latent\"], \n",
    "        layer_sizes=hyper[\"layer_sizes\"], \n",
    "        final_activation1=[Exp(), ClippedSoftplus()], \n",
    "        final_activation2=nn.Sigmoid(), \n",
    "        seed=182822,  \n",
    "    )\n",
    "    \n",
    "    #set up losses\n",
    "    criterion = QuadLoss()\n",
    "    \n",
    "    #set up optimizer\n",
    "    babel_opt = optim.Adam(list(babel_model.parameters()), lr=hyper[\"lr\"])\n",
    "    \n",
    "    #set up device \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    babel_model.to(device)\n",
    "\n",
    "    #training step\n",
    "    for epoch in range(hyper[\"nEpochs\"]):\n",
    "        train_losses = []\n",
    "        for idx, (rna_inputs, atac_inputs, nn_indices) in enumerate(train_loader):\n",
    "            babel_opt.zero_grad()\n",
    "\n",
    "            rna_inputs = Variable(rna_inputs).to(device)\n",
    "            atac_inputs = Variable(atac_inputs).to(device)\n",
    "            nn_indices = Variable(nn_indices).to(device)\n",
    "            inputs_list = [rna_inputs, atac_inputs]\n",
    "            retval11, retval12, retval21, retval22 = babel_model(inputs_list)\n",
    "            preds = (retval11, retval12, retval21, retval22)\n",
    "\n",
    "            train_loss = criterion(preds, inputs_list)\n",
    "            if hyper[\"add_hinge\"]:\n",
    "                hinge_loss = StructureHingeLoss(\n",
    "                    margin=0.2, \n",
    "                    max_val=1e6, \n",
    "                    lamb_match=hyper[\"lamb_match\"], \n",
    "                    lamb_nn=hyper[\"lamb_nn\"],\n",
    "                    device=device,\n",
    "                )\n",
    "                h_loss = hinge_loss(retval11[-1], retval22[-1], nn_indices)\n",
    "                #train_loss = train_loss + h_loss\n",
    "                train_loss = h_loss\n",
    "                #print(f\"hinge loss: {h_loss}\")\n",
    "                #print(f\"train loss with hinge: {train_loss}\")\n",
    "            train_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(babel_model.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "            babel_opt.step()\n",
    "\n",
    "            train_losses.append(train_loss.item())\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(np.mean(train_losses)))\n",
    "        \n",
    "        #evaluating step\n",
    "        with torch.no_grad():\n",
    "            babel_model.eval()\n",
    "            knn_acc = []\n",
    "            #mse_acc = []\n",
    "            for idx, samples in enumerate(test_loader):\n",
    "                rna_inputs = samples[0].float()\n",
    "                atac_inputs = samples[1].float()\n",
    "                rna_inputs = rna_inputs.to(device)\n",
    "                atac_inputs = atac_inputs.to(device)\n",
    "\n",
    "                inputs_list = [rna_inputs, atac_inputs]\n",
    "\n",
    "                retval11, retval12, retval21, retval22 = babel_model(inputs_list)\n",
    "                output_rna = retval11[-1]\n",
    "                output_atac = retval22[-1]\n",
    "                knn_acc.append(knn_criteria(rna_inputs.cpu().detach(), atac_inputs.cpu().detach(), \n",
    "                                            output_rna.cpu().detach(), output_atac.cpu().detach()))\n",
    "            avg_knn_auc = np.mean(knn_acc)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", acc: \" + str(avg_knn_auc))\n",
    "    test_knn_score, test_mse_score = model_eval(babel_model, test_data, device)\n",
    "    print(\"Final knn_auc: \" + str(test_knn_score))\n",
    "    print(\"Final mse: \" + str(test_mse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 1.1893136684711163\n",
      "Epoch: 0, acc: 0.2057571141841004\n",
      "Epoch: 5, train loss: 0.15159757549946123\n",
      "Epoch: 5, acc: 0.23212911920353294\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model_eval() missing 2 required positional arguments: 'test_adata' and 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-33122e9e3da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#test knn_auc plateau at around 0.09, seems that training starts to overfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_knn_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_knn_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mse_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_knn_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mse_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: model_eval() missing 2 required positional arguments: 'test_adata' and 'device'"
     ]
    }
   ],
   "source": [
    "#test knn_auc plateau at around 0.09, seems that training starts to overfit\n",
    "test_knn_score, test_mse_score = model_eval(test_data)\n",
    "print(test_knn_score)\n",
    "print(test_mse_score)\n",
    "train_knn_score, train_mse_score = model_eval(train_data)\n",
    "print(train_knn_score)\n",
    "print(train_mse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log the metrics\n",
    "path = \"{}Max_iter_{}lamb_anc_{}metrics.txt\".format(hyper[\"weightDirName\"], str(hyper[\"nEpochs\"]), str(hyper[\"lamb_anc\"]))\n",
    "'''torch.save({\n",
    "    \"num_iter\": hyper[\"nEpochs\"],\n",
    "    \"lamb_anc\": hyper[\"lamb_anc\"],\n",
    "    'knn_auc': knn_score,\n",
    "    'mse': mse_score,\n",
    "}, path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'a') as f:\n",
    "        print('nEpoch: ', hyper[\"nEpochs\"], 'lamb_anc:%.8f'%float(hyper[\"lamb_anc\"]) , ',knn_auc: %.8f' % float(knn_score), ', mse_score: %.8f' % float(mse_score), file=f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
