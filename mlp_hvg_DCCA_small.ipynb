{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes = load_raw.load_raw_cell_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcF3DwMRMGPl"
   },
   "source": [
    "**2. select the joint sub-datasets** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [],
   "source": [
    "scicar_data, joint_index, keep_cells_idx = load_raw.merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scicar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2029x7911 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 442844 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_scicar_data = load_raw.subset_joint_data(scicar_data, n_cells = 3000, n_genes = 8000)\n",
    "sub_train_data, sub_test_data = load_raw.train_test_split(sub_scicar_data)\n",
    "sub_train_data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<870x8000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19784 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_test_data.obsm[\"mode2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHbPxcQb0oBq"
   },
   "source": [
    "# **logcpm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e62BPkk84kPl"
   },
   "outputs": [],
   "source": [
    "#tryout log cpm scicar_data\n",
    "nm.log_cpm(scicar_data)\n",
    "nm.log_cpm(scicar_data, obsm = \"mode2\", obs = \"mode2_obs\", var = \"mode2_var\")\n",
    "nm.hvg_by_sc(scicar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uZadFiGYvkkx"
   },
   "outputs": [],
   "source": [
    "nm.hvg_by_sc(scicar_data,  obsm = \"mode2\", obs = \"mode2_obs\", var = \"mode2_var\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRA9D1KTwKIX",
    "outputId": "7f678584-9235-43fd-ce87-034bd8de8eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14671\n",
      "6057\n"
     ]
    }
   ],
   "source": [
    "print(len(scicar_data.uns[\"mode2_var\"]))\n",
    "print(len(scicar_data.var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "  def __init__(self, adata):\n",
    "    self.rna_data, self.atac_data = self._load_merge_data(adata)\n",
    "\n",
    "  def __len__(self):\n",
    "    #assert(len(self.rna_data) == len(self.atac_data))\n",
    "    return len(self.atac_data)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    rna_sample = self.rna_data.values[idx]\n",
    "    atac_sample = self.atac_data.values[idx]\n",
    "    #return a tensor that for a single observation\n",
    "    return {\"rna_tensor\": torch.from_numpy(rna_sample).float(), \"atac_tensor\": torch.from_numpy(atac_sample).float()}\n",
    "  \n",
    "  def _load_merge_data(self, adata):\n",
    "    rna_df = pd.DataFrame(data = adata.X.toarray(), index = np.array(adata.obs.index), columns = np.array(adata.var.index))\n",
    "    atac_df = pd.DataFrame(data = adata.obsm[\"mode2\"].toarray(), index = np.array(adata.uns[\"mode2_obs\"]), columns = np.array(adata.uns[\"mode2_var\"]))\n",
    "    return rna_df, atac_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4szWON2-cpvS"
   },
   "source": [
    "# **Compute DCCA loss (-corr(H1, H2))**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Q-6pKoruco26"
   },
   "outputs": [],
   "source": [
    "class cca_loss():\n",
    "  def __init__(self, out_dim, device, use_all_singvals=False):\n",
    "    self.out_dim = out_dim #parameter o in original paper\n",
    "    self.use_all_singvals = use_all_singvals\n",
    "    self.device = device\n",
    "  \n",
    "  def loss(self, H1, H2):\n",
    "    r1 = 1e-3\n",
    "    r2 = 1e-3\n",
    "    eps = 1e-9\n",
    "\n",
    "    #transpose H1, H2: m x o -> o x m\n",
    "    H1 = H1.t()\n",
    "    H2 = H2.t()\n",
    "    #assert torch.isnan(H1).sum().item() == 0\n",
    "    #assert torch.isnan(H2).sum().item() == 0\n",
    "\n",
    "    m = H1.size(1)\n",
    "    o1, o2 = H1.size(0), H2.size(0)\n",
    "\n",
    "    #produce the centered data matrices: H1 - 1/m*H1Â·I (same for H2bar)\n",
    "    H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n",
    "    H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n",
    "    assert torch.isnan(H1bar).sum().item() == 0\n",
    "    assert torch.isnan(H2bar).sum().item() == 0\n",
    "\n",
    "    SigmaHat12 = (1.0/(m-1))*torch.matmul(H1bar, H2bar.t())\n",
    "    SigmaHat11 = (1.0/(m-1))*torch.matmul(H1bar, H1bar.t()) + r1*torch.eye(o1, device=self.device)\n",
    "    SigmaHat22 = (1.0/(m-1))*torch.matmul(H2bar, H2bar.t()) + r2*torch.eye(o2, device=self.device)\n",
    "    #assert torch.isnan(SigmaHat11).sum().item() == 0\n",
    "    #assert torch.isnan(SigmaHat12).sum().item() == 0\n",
    "    #assert torch.isnan(SigmaHat22).sum().item() == 0\n",
    "\n",
    "    #calculate the root inverse (e.g. SigmaHat11^(-1/2)) using sigular value decomposition\n",
    "    D1, V1 = torch.symeig(SigmaHat11, eigenvectors=True)\n",
    "    D2, V2 = torch.symeig(SigmaHat22, eigenvectors=True)\n",
    "\n",
    "    # ??? probably problemetic in gene count setting\n",
    "    posIdx1 = torch.gt(D1, eps).nonzero()[:, 0]\n",
    "    D1 = D1[posIdx1]\n",
    "    V1 = V1[:, posIdx1]\n",
    "\n",
    "    posIdx2 = torch.gt(D2, eps).nonzero()[:, 0]\n",
    "    D2 = D2[posIdx2]\n",
    "    V2 = V2[:,posIdx2]\n",
    "\n",
    "    #???take care of torch.sqrt\n",
    "    SigmaHatRootInv11 = torch.matmul(torch.matmul(V1, torch.diag((D1)**(-0.5))), V1.t())\n",
    "    SigmaHatRootInv22 = torch.matmul(torch.matmul(V2, torch.diag((D2)**(-0.5))), V2.t())\n",
    "\n",
    "    #calculate T\n",
    "    Tval = torch.matmul(torch.matmul(SigmaHatRootInv11, SigmaHat12), SigmaHatRootInv22)\n",
    "\n",
    "    #calculate corr(H1, H2): matrix trace norm of T or sum of top k singular vals of T\n",
    "    trace_TT = torch.matmul(Tval.t(), Tval)\n",
    "    if self.use_all_singvals:\n",
    "      corr = torch.trace(torch.sqrt(trace_TT))\n",
    "      #assert torch.isnan(corr).item() == 0\n",
    "      \n",
    "    else:\n",
    "      trace_TT = torch.add(trace_TT, (torch.eye(trace_TT.shape[0])*r1).to(self.device))\n",
    "      U, V = torch.symeig(trace_TT, eigenvectors=True)\n",
    "      U = torch.where(U>eps, U, (torch.ones(U.shape).double()*eps).to(self.device))\n",
    "      U = U.topk(self.out_dim)[0]\n",
    "      corr = torch.sum(torch.sqrt(U))\n",
    "    #print(\"loss: \" + str(-corr))\n",
    "    return -corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Reconstruction Loss Using Decoder for each AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(x, x_recon, y, y_recon):\n",
    "    MSE = nn.MSELoss()\n",
    "    x_loss = MSE(x, x_recon)\n",
    "    y_loss = MSE(y, y_recon)\n",
    "    total_loss = x_loss + y_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models(now just encoder net) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "46TuWH_Rgwnc"
   },
   "outputs": [],
   "source": [
    "class EN_NET(nn.Module):\n",
    "    def __init__(self, n_input, n_out, layer_sizes, use_decode=False):\n",
    "        super(EN_NET, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_out = n_out\n",
    "        self.encoder_layers = []\n",
    "        self.layer_sizes = [n_input] + layer_sizes + [n_out]\n",
    "        self.use_decode = use_decode\n",
    "        \n",
    "        for layer_idx in range(len(self.layer_sizes)-1):\n",
    "            if layer_idx == len(self.layer_sizes) - 2:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "            else:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *self.encoder_layers\n",
    "        )\n",
    "    \n",
    "        self.decoder_layers = []\n",
    "        for layer_idx in range(len(self.layer_sizes)-1, 0, -1):\n",
    "            if layer_idx == 1:\n",
    "                self.decoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx-1]))\n",
    "            else:\n",
    "                self.decoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx-1]))\n",
    "                self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx-1]))\n",
    "                self.decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(\n",
    "            *self.decoder_layers\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #calculate std from log(var)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "        return z, x_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq4Q758AZn3W"
   },
   "source": [
    "# Assembly Neural Net And Loss Into DCCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yiiicKn7ZaVo"
   },
   "outputs": [],
   "source": [
    "class DCCA(nn.Module):\n",
    "  def __init__(self, n_input1, n_input2, layer_sizes1, layer_sizes2, \n",
    "               n_out, use_all_singvals=False, device=torch.device(\"cpu\"), use_decode = False):\n",
    "    super(DCCA, self).__init__()\n",
    "    self.Net1 = EN_NET(n_input1, n_out, layer_sizes1, use_decode = use_decode).double()\n",
    "    self.Net2 = EN_NET(n_input2, n_out, layer_sizes2, use_decode = use_decode).double()\n",
    "    self.cca_loss = cca_loss(out_dim=n_out, use_all_singvals=use_all_singvals, device=device).loss\n",
    "    self.recon_loss = recon_loss\n",
    "    self.device = device\n",
    "    self.use_decode = use_decode\n",
    "  def forward(self, x1, x2):\n",
    "    z1, x1_recon = self.Net1(x1)\n",
    "    z2, x2_recon = self.Net2(x2)\n",
    "    return z1, x1_recon, z2, x2_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsbRfruLdE2j"
   },
   "source": [
    "# **Train Basic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0AH8OM02c71Y"
   },
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":150,\n",
    "    \"dimRNA\":7911,\n",
    "    \"dimATAC\":8000,\n",
    "    \"n_hidden\":1024,\n",
    "    \"n_out\":100,\n",
    "    \"batchSize\":128,\n",
    "    \"lr\":1e-3,\n",
    "    \"weightDirName\": './checkpoint/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "train_set = Merge_Dataset(sub_train_data)\n",
    "test_set = Merge_Dataset(sub_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_HZZgU6o2Vv5"
   },
   "outputs": [],
   "source": [
    "#use GPU if available\n",
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "qcs2gy4KwzCL"
   },
   "outputs": [],
   "source": [
    "#load data loader\n",
    "train_loader = DataLoader(train_set, batch_size=hyper[\"batchSize\"], drop_last=False, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=hyper[\"batchSize\"], drop_last=False, shuffle=False)\n",
    "\n",
    "#load basic models\n",
    "toy_model_recon = DCCA(n_input1=hyper[\"dimRNA\"], \n",
    "                 n_input2=hyper[\"dimATAC\"], \n",
    "                 layer_sizes1=[2048, 1024, 256], \n",
    "                 layer_sizes2=[4096, 2048, 512], \n",
    "                 n_out=hyper[\"n_out\"],\n",
    "                 use_all_singvals=False,\n",
    "                 device=torch.device(my_device), \n",
    "                 use_decode = True)\n",
    "toy_model = DCCA(n_input1=hyper[\"dimRNA\"], \n",
    "                 n_input2=hyper[\"dimATAC\"], \n",
    "                 layer_sizes1=[2048, 1024, 256], \n",
    "                 layer_sizes2=[4096, 2048, 512], \n",
    "                 n_out=hyper[\"n_out\"],\n",
    "                 use_all_singvals=False,\n",
    "                 device=torch.device(my_device), \n",
    "                 use_decode = False)\n",
    "\n",
    "#set up optimizer\n",
    "optimizer_recon = optim.Adam(list(toy_model_recon.parameters()), lr=hyper[\"lr\"])\n",
    "optimizer = optim.Adam(list(toy_model.parameters()), lr=hyper[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "lamb_recon = 10\n",
    "#set up train functions\n",
    "def train(model, optimizer, epoch):  \n",
    "    #print(\"Epoch:\"+str(epoch))\n",
    "    train_losses = []\n",
    "    model.to(model.device)\n",
    "    model.train()\n",
    "    for idx, samples in enumerate(train_loader):\n",
    "        rna_inputs = samples[\"rna_tensor\"].double()\n",
    "        atac_inputs = samples[\"atac_tensor\"].double()\n",
    "        rna_inputs, atac_inputs = Variable(rna_inputs), Variable(atac_inputs)\n",
    "        rna_inputs = rna_inputs.to(my_device)\n",
    "        atac_inputs = atac_inputs.to(my_device)\n",
    "        optimizer.zero_grad()\n",
    "    z_rna, rna_recon, z_atac, atac_recon = model(rna_inputs, atac_inputs)\n",
    "    #print(\"before loss calculated\"\n",
    "    if model.use_decode:\n",
    "        loss_epoch = model.cca_loss(z_rna, z_atac)+\\\n",
    "        lamb_recon*model.recon_loss(rna_inputs, rna_recon, atac_inputs, atac_recon)\n",
    "    else:\n",
    "        loss_epoch = model.cca_loss(z_rna, z_atac)\n",
    "    train_losses.append(loss_epoch.item())\n",
    "    #print(\"after loss calculated\")\n",
    "    loss_epoch.backward()\n",
    "    optimizer.step()\n",
    "      \n",
    "    #loss functions for each modalities\n",
    "    train_loss = np.mean(train_losses)\n",
    "    if epoch % 15 == 0:\n",
    "        print(\"Epoch:\"+str(epoch) + \", loss: \" + str(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Mc1swrmXm7sZ"
   },
   "outputs": [],
   "source": [
    "def knn_criteria(rna_inputs, atac_inputs, rna_outputs, atac_outputs, proportion_neighbors=0.1, n_svd=100):\n",
    "  n_svd = min([n_svd, min(rna_inputs.shape)-1])\n",
    "  n_neighbors = int(np.ceil(proportion_neighbors*rna_inputs.shape[0]))\n",
    "  X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "  _, indices_true = (\n",
    "      sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(rna_inputs).kneighbors(rna_inputs)\n",
    "  )\n",
    "  _, indices_pred = (\n",
    "      sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(rna_outputs).kneighbors(atac_outputs)\n",
    "  )\n",
    "  neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "  for i in range(rna_inputs.shape[0]):\n",
    "    _, pred_matches, true_matches = np.intersect1d(\n",
    "        indices_pred[i], indices_true[i], return_indices=True\n",
    "    )\n",
    "    neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "    neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "  neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * rna_inputs.shape[0])\n",
    "  area_under_curve = np.mean(neighbors_match_curve)\n",
    "  return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Mz5ZLnKCZUEt"
   },
   "outputs": [],
   "source": [
    "def test_model(model, epoch, test_loader, device):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    knn_acc = []\n",
    "    #mse_acc = []\n",
    "    for idx, samples in enumerate(test_loader):\n",
    "      rna_inputs = samples[\"rna_tensor\"].double()\n",
    "      atac_inputs = samples[\"atac_tensor\"].double()\n",
    "      rna_inputs = rna_inputs.to(device)\n",
    "      atac_inputs = atac_inputs.to(device)\n",
    "\n",
    "      output_rna, rna_recon, output_atac, atac_recon = model(rna_inputs, atac_inputs)\n",
    "      knn_acc.append(knn_criteria(rna_inputs.cpu().detach(), atac_inputs.cpu().detach(), \n",
    "                                  output_rna.cpu().detach(), output_atac.cpu().detach()))\n",
    "      avg_knn_acc = np.mean(knn_acc)\n",
    "    if epoch % 15 == 0:\n",
    "      print(\"Epoch:\"+str(epoch) + \", average knn_acc: \" + str(avg_knn_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0nHEL8K7EZ0",
    "outputId": "33037275-08dd-443b-b305-783fcc90998b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, loss: -39.8210688068998\n",
      "Epoch:0, average knn_acc: 0.035752647358137456\n",
      "Epoch:15, loss: -43.063214741922735\n",
      "Epoch:15, average knn_acc: 0.02855416426322772\n",
      "Epoch:30, loss: -56.12710590377688\n",
      "Epoch:30, average knn_acc: 0.07420931775056512\n",
      "Epoch:45, loss: -50.70404403312616\n",
      "Epoch:45, average knn_acc: 0.07509425670398429\n",
      "Epoch:60, loss: -33.93516992833749\n",
      "Epoch:60, average knn_acc: 0.10958923150199244\n",
      "Epoch:75, loss: -75.50576073956583\n",
      "Epoch:75, average knn_acc: 0.12849309466151954\n",
      "Epoch:90, loss: -63.640180220805945\n",
      "Epoch:90, average knn_acc: 0.09711358488441928\n",
      "Epoch:105, loss: -60.66865043737877\n",
      "Epoch:105, average knn_acc: 0.0910031172028366\n",
      "Epoch:120, loss: -71.93848158322807\n",
      "Epoch:120, average knn_acc: 0.08253924481687988\n",
      "Epoch:135, loss: -58.76421733635981\n",
      "Epoch:135, average knn_acc: 0.10557101002145386\n",
      "Epoch:0, loss: -54.47945434745719\n",
      "Epoch:0, average knn_acc: 0.09997569078301076\n",
      "Epoch:15, loss: -66.52064460575556\n",
      "Epoch:15, average knn_acc: 0.03210014048789244\n",
      "Epoch:30, loss: -74.9293234577589\n",
      "Epoch:30, average knn_acc: 0.12981209630507026\n",
      "Epoch:45, loss: -81.33537760785767\n",
      "Epoch:45, average knn_acc: 0.08554009622746721\n",
      "Epoch:60, loss: -80.19205039499268\n",
      "Epoch:60, average knn_acc: 0.12501392947325693\n",
      "Epoch:75, loss: -83.25956682796547\n",
      "Epoch:75, average knn_acc: 0.09467372567219642\n",
      "Epoch:90, loss: -85.14934641506726\n",
      "Epoch:90, average knn_acc: 0.12087467553694799\n",
      "Epoch:105, loss: -84.75764030358198\n",
      "Epoch:105, average knn_acc: 0.08966272790722697\n",
      "Epoch:120, loss: -84.0469300439184\n",
      "Epoch:120, average knn_acc: 0.12057829968524514\n",
      "Epoch:135, loss: -84.67228575622534\n",
      "Epoch:135, average knn_acc: 0.14475291749360739\n"
     ]
    }
   ],
   "source": [
    "max_iter = hyper[\"nEpochs\"]\n",
    "#train model with reconstruction loss\n",
    "for epoch in range(max_iter):\n",
    "    train(toy_model_recon, optimizer_recon, epoch)\n",
    "    test_model(toy_model_recon, epoch, test_loader, my_device)\n",
    "    \n",
    "#train model without reconstruction loss\n",
    "for epoch in range(max_iter):\n",
    "    train(toy_model, optimizer, epoch)\n",
    "    test_model(toy_model, epoch, test_loader, my_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LFEBScjW5Bu",
    "outputId": "ca4244af-30e6-4468-c50e-b578a43c4685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07190401780670187\n",
      "1.0026839700117922\n"
     ]
    }
   ],
   "source": [
    "#performance for reconstruct model\n",
    "atac_inputs = Variable(torch.from_numpy(test_set.atac_data.values).double()).to(my_device)\n",
    "rna_inputs = Variable(torch.from_numpy(test_set.rna_data.values).double()).to(my_device)\n",
    "toy_model_recon.eval()\n",
    "out_rna, rna_recon, out_atac, atac_recon= toy_model_recon(rna_inputs, atac_inputs)\n",
    "sub_test_data.obsm[\"aligned\"] = sparse.csr_matrix(out_rna.cpu().detach())\n",
    "sub_test_data.obsm[\"mode2_aligned\"] = sparse.csr_matrix(out_atac.cpu().detach())\n",
    "knn_auc_recon = metrics.knn_auc(sub_test_data)\n",
    "mse_recon = metrics.mse(sub_test_data)\n",
    "print(knn_auc_recon)\n",
    "print(mse_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0851978985392798\n",
      "1.0015561559198058\n"
     ]
    }
   ],
   "source": [
    "#performances for non-recon model\n",
    "toy_model.eval()\n",
    "out_rna, _, out_atac, _= toy_model(rna_inputs, atac_inputs)\n",
    "sub_test_data.obsm[\"aligned\"] = sparse.csr_matrix(out_rna.cpu().detach())\n",
    "sub_test_data.obsm[\"mode2_aligned\"] = sparse.csr_matrix(out_atac.cpu().detach())\n",
    "knn_auc = metrics.knn_auc(sub_test_data)\n",
    "mse = metrics.mse(sub_test_data)\n",
    "print(knn_auc)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "-mZFHO0j5vwe",
    "outputId": "af4818e5-4905-4f38-ff28-98646d8a7b73",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-12a5d4ce04f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                  \u001b[0muse_all_singvals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                  device=torch.device(my_device))\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcheckpoint_out64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_out64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel_out64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_out64\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_out64' is not defined"
     ]
    }
   ],
   "source": [
    "model_out = DCCA(n_input1=hyper[\"dimRNA\"], \n",
    "                 n_input2=hyper[\"dimATAC\"], \n",
    "                 layer_sizes1=[2048, 1024, 256], \n",
    "                 layer_sizes2=[4096, 2048, 512], \n",
    "                 n_out=hyper[\"n_out\"],\n",
    "                 use_all_singvals=False,\n",
    "                 device=torch.device(my_device))\n",
    "checkpoint_out64 = torch.load(path_out64)\n",
    "model_out64.load_state_dict(checkpoint_out64[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97PHeKh1TUL9"
   },
   "outputs": [],
   "source": [
    "path_out100 = \"\"\n",
    "torch.save({\n",
    "    \"num_iter\": hyper[\"nEpochs\"],\n",
    "    \"n_out\": hyper[\"n_out\"],\n",
    "    \"model_state_dict\": toy_model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"knn_auc\": 9.643159972821067e-06,\n",
    "    \"mse\": 0.985969183721269\n",
    "}, path_out100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mlp_hvg_DCCA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
