{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayueg/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370117127/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train_mod1 = ad.read_h5ad(\"./matching/sample_multitome/openproblems_bmmc_multiome_starter.train_mod1.h5ad\")\n",
    "input_train_mod2 = ad.read_h5ad(\"./matching/sample_multitome/openproblems_bmmc_multiome_starter.train_mod2.h5ad\")\n",
    "input_train_sol = ad.read_h5ad(\"./matching/sample_multitome/openproblems_bmmc_multiome_starter.train_sol.h5ad\")\n",
    "input_test_mod1 = ad.read_h5ad(\"./matching/sample_multitome/openproblems_bmmc_multiome_starter.test_mod1.h5ad\")\n",
    "input_test_mod2 = ad.read_h5ad(\"./matching/sample_multitome/openproblems_bmmc_multiome_starter.test_mod2.h5ad\")\n",
    "input_test_sol = ad.read_h5ad(\"./matching/sample_multitome/openproblems_bmmc_multiome_starter.test_sol.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\": 20,\n",
    "    \"dimRNA\": input_train_mod1.X.shape[1],\n",
    "    \"dimATAC\": input_train_mod2.shape[1],\n",
    "    \"train_nobs\": input_train_mod1.X.shape[0],\n",
    "    \"test_nobs\": input_test_mod1.X.shape[0],\n",
    "    \"layer_sizes\": [1024, 512, 256],\n",
    "    \"nz\": 64,\n",
    "    \"batchSize\": 512,\n",
    "    \"lr\": 1e-3,\n",
    "    \"lamb_kl\": 1e-9,\n",
    "    \"lamb_anc\": 1e-9,\n",
    "    \"clip_grad\": 0.1,\n",
    "    \"checkpoint_path\": './checkpoint/vae_matching.pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, adata_mod1, adata_mod2, labels):\n",
    "        #self.rna_data_filtered, self.atac_data_filtered = self._load_merge_data(adata_filtered)\n",
    "        #self.rna_data_raw = self._load_raw_ref_data(adata_raw)\n",
    "        self.rna_data = adata_mod1.X.toarray()\n",
    "        self.atac_data = adata_mod2.X.toarray()\n",
    "        self.labels = labels.X.toarray()\n",
    "        self.idx_map = []\n",
    "        \n",
    "        assert self.rna_data.shape[0] == self.atac_data.shape[0]\n",
    "        self.n_obs = self.rna_data.shape[0]\n",
    "        for i in range(self.n_obs):\n",
    "            for j in range(self.n_obs):\n",
    "                self.idx_map.append((i, j))\n",
    "        \n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.idx_map)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.idx_map[idx]\n",
    "        rna_sample = torch.from_numpy(self.rna_data[i]).float()\n",
    "        atac_sample = torch.from_numpy(self.atac_data[j]).float()\n",
    "        label_sample = self.labels[i][j]\n",
    "        #return a tensor that for a single observation\n",
    "        return rna_sample, atac_sample, label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, adata_mod1, adata_mod2):\n",
    "        #self.rna_data_filtered, self.atac_data_filtered = self._load_merge_data(adata_filtered)\n",
    "        #self.rna_data_raw = self._load_raw_ref_data(adata_raw)\n",
    "        self.rna_data = adata_mod1.X.toarray()\n",
    "        self.atac_data = adata_mod2.X.toarray()\n",
    "        self.idx_map = []\n",
    "        \n",
    "        assert self.rna_data.shape[0] == self.atac_data.shape[0]\n",
    "        self.n_obs = self.rna_data.shape[0]\n",
    "        for i in range(self.n_obs):\n",
    "            for j in range(self.n_obs):\n",
    "                self.idx_map.append((i, j))\n",
    "        \n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.idx_map)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.idx_map[idx]\n",
    "        rna_sample = torch.from_numpy(self.rna_data[i]).float()\n",
    "        atac_sample = torch.from_numpy(self.atac_data[j]).float()\n",
    "        return rna_sample, atac_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models(autoencoders) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "46TuWH_Rgwnc"
   },
   "outputs": [],
   "source": [
    "class FC_VAE(nn.Module):\n",
    "    def __init__(self, n_input, nz, layer_sizes=hyper[\"layer_sizes\"]):\n",
    "        super(FC_VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.nz = nz\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        self.encoder_layers = []\n",
    "\n",
    "        self.encoder_layers.append(nn.Linear(n_input, self.layer_sizes[0]))\n",
    "        self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "        self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[0]))\n",
    "\n",
    "        for layer_idx in range(len(layer_sizes)-1):\n",
    "            if layer_idx == len(layer_sizes) - 2:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "            else:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *self.encoder_layers\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "        self.fc2 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "        self.decoder_layers.append(nn.Linear(nz, self.layer_sizes[-1]))\n",
    "        self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[-1]))\n",
    "        self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        for layer_idx in range(len(self.layer_sizes)-1, 0, -1):\n",
    "            self.decoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx-1]))\n",
    "            self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx-1]))\n",
    "            self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.decoder_layers.append(nn.Linear(self.layer_sizes[0], self.n_input))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            *self.decoder_layers\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc1(h), self.fc2(h)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #calculate std from log(var)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        res = self.decode(z)\n",
    "        return res, z, mu, logvar\n",
    "\n",
    "    def get_latent_var(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement contrastive loss\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, margin=0.1, pos_coef=10):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pos_coef = pos_coef\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        pdist = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "        loss_contrastive = torch.mean(self.pos_coef * (1 - label) * torch.pow(pdist, 2) + \n",
    "                                      label * torch.pow(torch.clamp(self.margin - pdist, min = 0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "def get_data_loaders(train_mod1, train_mod2, train_sol, test_mod1, test_mod2):\n",
    "    train_set = TrainDataset(train_mod1, train_mod2, train_sol)\n",
    "    test_set = TestDataset(test_mod1, test_mod2)\n",
    "    \n",
    "    #load data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_set, \n",
    "        batch_size=hyper[\"batchSize\"], \n",
    "        drop_last=False, \n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, \n",
    "        batch_size=hyper[\"batchSize\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "LH_uoNVQ6v1O"
   },
   "outputs": [],
   "source": [
    "#set up loss function\n",
    "def basic_loss(recon_x, x, mu, logvar, lamb1):\n",
    "    MSE = nn.MSELoss()\n",
    "    lloss = MSE(recon_x, x)\n",
    "    #KL divergence\n",
    "    KL_loss = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    lloss = lloss + lamb1*KL_loss\n",
    "    return lloss\n",
    "\n",
    "#anchor loss for minimizing distance between paired observation\n",
    "def anchor_loss(embed_rna, embed_atac):\n",
    "    L1 = nn.L2Loss()\n",
    "    anc_loss = L2(embed_rna, embed_atac)\n",
    "    return anc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_score(rna_output, atac_output):\n",
    "    cos_score = F.cosine_similarity(rna_output, atac_output)\n",
    "    return(cos_score.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def main():\n",
    "    #load training data and testing data\n",
    "    train_loader, test_loader = get_data_loaders(\n",
    "        input_train_mod1,\n",
    "        input_train_mod2,\n",
    "        input_train_sol,\n",
    "        input_test_mod1,\n",
    "        input_test_mod2,\n",
    "    )\n",
    "    \n",
    "    #load checkpoint\n",
    "    checkpoint = None\n",
    "    if path.exists(hyper[\"checkpoint_path\"]):\n",
    "        checkpoint = torch.load(hyper[\"checkpoint_path\"])\n",
    "    \n",
    "    #load basic models\n",
    "    netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "    netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "    if checkpoint != None:\n",
    "        netRNA.load_state_dict(checkpoint[\"net_rna_state_dict\"])\n",
    "        netATAC.load_state_dict(checkpoint[\"net_atac_state_dict\"])\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"using GPU\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    netRNA.to(device)\n",
    "    netATAC.to(device)\n",
    "    \n",
    "    #set up additional criterion (constrastive...)\n",
    "    criterion_contrastive = ContrastiveLoss()\n",
    "    \n",
    "    #setup optimizers for two nets\n",
    "    opt_netRNA = optim.Adam(list(netRNA.parameters()), lr=hyper[\"lr\"])\n",
    "    opt_netATAC = optim.Adam(list(netATAC.parameters()), lr=hyper[\"lr\"])\n",
    "    scheduler_netRNA = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt_netRNA,\n",
    "        patience=5,\n",
    "        threshold=0.01,\n",
    "        mode=\"max\",\n",
    "        min_lr=1e-5,\n",
    "    )\n",
    "    scheduler_netATAC = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt_netATAC,\n",
    "        patience=5,\n",
    "        threshold=0.01,\n",
    "        mode=\"max\",\n",
    "        min_lr=1e-5,\n",
    "    )\n",
    "    \n",
    "    best_match_score = 0\n",
    "    test_sol = torch.from_numpy(input_test_sol.X.toarray()).float()\n",
    "    train_sol = torch.from_numpy(input_train_sol.X.toarray()).float()\n",
    "    \n",
    "    if checkpoint != None:\n",
    "        best_match_score = checkpoint[\"dev_match_score\"]\n",
    "        \n",
    "    #training\n",
    "    for epoch in range(hyper[\"nEpochs\"]):\n",
    "        train_losses = []\n",
    "        train_scores = []\n",
    "        netRNA.train()\n",
    "        netATAC.train()\n",
    "        #train for epochs\n",
    "        for idx, (rna_inputs, atac_inputs, label) in enumerate(train_loader):\n",
    "            opt_netATAC.zero_grad()\n",
    "            opt_netRNA.zero_grad()\n",
    "            rna_inputs = Variable(rna_inputs).to(device)\n",
    "            atac_inputs = Variable(atac_inputs).to(device)\n",
    "            \n",
    "            recon_rna, z_rna, mu_rna, logvar_rna = netRNA(rna_inputs)\n",
    "            recon_atac, z_atac, mu_atac, logvar_atac = netATAC(atac_inputs)\n",
    "            #rna_loss = basic_loss(recon_rna, rna_inputs, mu_rna, logvar_rna, lamb1=hyper[\"lamb_kl\"])\n",
    "            atac_loss = basic_loss(recon_atac, atac_inputs, mu_atac, logvar_atac, lamb1=hyper[\"lamb_kl\"])\n",
    "            contrastive_loss = criterion_contrastive(z_rna, z_atac, label)\n",
    "            \n",
    "            '''if epoch % 10 == 0:\n",
    "                print(f\"rna_loss: {rna_loss}\")\n",
    "                print(f\"atac_loss:{atac_loss}\")\n",
    "                print(f\"contrastive loss: {contrastive_loss}\")'''\n",
    "\n",
    "            #loss functions for each modalities\n",
    "            train_loss = atac_loss + contrastive_loss\n",
    "            \n",
    "            #train_loss = rna_loss + atac_loss\n",
    "            #train_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss\n",
    "            #rain_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss + h_loss\n",
    "            train_loss.backward()\n",
    "            #nn.utils.clip_grad_norm_(netRNA.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "            #nn.utils.clip_grad_norm_(netATAC.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "            opt_netRNA.step()\n",
    "            opt_netATAC.step()\n",
    "            train_losses.append(train_loss.item())\n",
    "            \n",
    "            scores_batch = similar_score(z_rna, z_atac)\n",
    "            train_scores.append(scores_batch.reshape((len(scores_batch), 1)))\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        train_score_matrix = torch.tensor(np.vstack(train_scores).reshape((hyper[\"train_nobs\"], hyper[\"train_nobs\"])))\n",
    "        train_socre_matrix = train_score_matrix / torch.sum(train_score_matrix, axis = 1)\n",
    "        #train_score_matrix = F.softmax(torch.tensor(train_score_matrix), dim=1)\n",
    "        train_match_score = torch.sum(train_score_matrix * train_sol)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(avg_train_loss))\n",
    "            print(\"Epoch: \" + str(epoch) + \", train similarity score: \" + str(train_match_score))\n",
    "        \n",
    "        #evaluating step\n",
    "        netRNA.eval()\n",
    "        netATAC.eval()\n",
    "        scores = []\n",
    "        with torch.no_grad():\n",
    "            for idx, samples in enumerate(test_loader):\n",
    "                rna_inputs= samples[0].float().to(device)\n",
    "                atac_inputs = samples[1].float().to(device)\n",
    "\n",
    "                _, output_rna, _, _ = netRNA(rna_inputs)\n",
    "                _, output_atac, _, _ = netATAC(atac_inputs)\n",
    "                scores_batch = similar_score(output_rna, output_atac)\n",
    "                scores.append(scores_batch.reshape((len(scores_batch), 1)))\n",
    "                \n",
    "        score_matrix = torch.tensor(np.vstack(scores).reshape((hyper[\"test_nobs\"], hyper[\"test_nobs\"])))\n",
    "        score_matrix = score_matrix / torch.sum(score_matrix, axis = 1)\n",
    "        if epoch == 10:\n",
    "            print(score_matrix)\n",
    "        dev_match_score = torch.sum(score_matrix * test_sol)\n",
    "        \n",
    "        if dev_match_score > best_match_score:\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"clip_grad\": hyper['clip_grad'],\n",
    "                \"layer_sizes\": hyper['layer_sizes'],\n",
    "                \"lr\": hyper[\"lr\"],\n",
    "                \"net_rna_state_dict\": netRNA.state_dict(),\n",
    "                \"net_atac_state_dict\": netATAC.state_dict(),\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"dev_match_score\": dev_match_score,\n",
    "            }, hyper[\"checkpoint_path\"])\n",
    "                \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", dev similarity score: \" + str(dev_match_score))\n",
    "        \n",
    "        scheduler_netRNA.step(dev_match_score)\n",
    "        scheduler_netATAC.step(dev_match_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 1.5615716874771217\n",
      "Epoch: 0, train similarity score: tensor(187.1886)\n",
      "Epoch: 0, dev similarity score: tensor(1.0003)\n",
      "Epoch: 5, train loss: 0.045573747396224835\n",
      "Epoch: 5, train similarity score: tensor(241.1765)\n",
      "Epoch: 5, dev similarity score: tensor(1.0262)\n",
      "Epoch: 10, train loss: 0.1644353379999272\n",
      "Epoch: 10, train similarity score: tensor(227.8022)\n",
      "tensor([[ 4.7032e-03,  9.4605e-04,  2.1284e-03,  ...,  5.6483e-03,\n",
      "          5.3538e-03, -1.4571e-04],\n",
      "        [ 5.0714e-03,  1.8244e-04, -5.0495e-05,  ...,  6.0322e-03,\n",
      "          5.7575e-03, -6.2041e-04],\n",
      "        [ 4.7388e-03, -7.1514e-04,  1.8912e-03,  ...,  5.7369e-03,\n",
      "          5.3974e-03,  2.1532e-03],\n",
      "        ...,\n",
      "        [ 4.8433e-03,  1.0796e-03, -9.8414e-04,  ...,  5.8315e-03,\n",
      "          5.5583e-03,  7.6700e-04],\n",
      "        [ 4.8757e-03, -1.1154e-03,  4.1089e-04,  ...,  5.8013e-03,\n",
      "          5.5328e-03, -7.0081e-04],\n",
      "        [ 5.1629e-03, -3.0785e-04,  2.0870e-03,  ...,  6.2070e-03,\n",
      "          5.9521e-03, -3.9464e-04]])\n",
      "Epoch: 10, dev similarity score: tensor(1.0056)\n",
      "Epoch: 15, train loss: 0.0054988092590184485\n",
      "Epoch: 15, train similarity score: tensor(247.8568)\n",
      "Epoch: 15, dev similarity score: tensor(1.0001)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(netRNA, netATAC, test_data_filtered, test_data_raw, title):\n",
    "    netRNA.eval()\n",
    "    netATAC.eval()\n",
    "    rna_inputs = Variable(torch.from_numpy(test_data_filtered.X.toarray()).float())\n",
    "    atac_inputs = Variable(torch.from_numpy(test_data_filtered.obsm[\"mode2\"].toarray()).float())\n",
    "    if torch.cuda.is_available():\n",
    "        rna_inputs = rna_inputs.cuda()\n",
    "        atac_inputs = atac_inputs.cuda()\n",
    "    _, z_rna, _, _ = netRNA(rna_inputs)\n",
    "    _, z_atac, _, _ = netATAC(atac_inputs)\n",
    "    test_data_raw.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "    test_data_raw.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "    metrics.plot_multimodal_umap(test_data_raw, title=title, num_points=100, connect_modalities=True)\n",
    "    knn_score, mse_score = metrics.knn_auc(test_data_raw), metrics.mse(test_data_raw)\n",
    "    return knn_score, mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-962f2c9b464e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#plot UMAP result and show evaluation metrics value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetRNA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetATAC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"VAE with Structure-Preserving Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "#load checkpoint\n",
    "checkpoint=None\n",
    "if path.exists(hyper[\"checkpoint_path\"]):\n",
    "    checkpoint = torch.load(hyper[\"checkpoint_path\"], map_location=\"cpu\")\n",
    "netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "if checkpoint != None:\n",
    "    netRNA.load_state_dict(checkpoint[\"net_rna_state_dict\"])\n",
    "    netATAC.load_state_dict(checkpoint[\"net_atac_state_dict\"])\n",
    "    \n",
    "#plot UMAP result and show evaluation metrics value\n",
    "model_eval(netRNA, netATAC, test_data_filtered, test_data_raw, title=\"VAE with Structure-Preserving Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similairty(netRNA, netATAC, test_data_filtered):\n",
    "    netRNA.eval()\n",
    "    netATAC.eval()\n",
    "    rna_inputs = Variable(torch.from_numpy(test_data_filtered.X.toarray()).float())\n",
    "    atac_inputs = Variable(torch.from_numpy(test_data_filtered.obsm[\"mode2\"].toarray()).float())\n",
    "    if torch.cuda.is_available():\n",
    "        rna_inputs = rna_inputs.cuda()\n",
    "        atac_inputs = atac_inputs.cuda()\n",
    "    _, z_rna, _, _ = netRNA(rna_inputs)\n",
    "    _, z_atac, _, _ = netATAC(atac_inputs)\n",
    "    cos_score = nn.CosineSimilairty(z_rna, z_atac)\n",
    "    return(cos_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
